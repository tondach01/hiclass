{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69ec9bec-43c3-4b6d-a1a6-b68ce50ac711",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from load import Dataset\n",
    "\n",
    "def add_nominal_noise(df, percentage):\n",
    "    noisy_df = df.copy()\n",
    "    nominal_columns = df.select_dtypes(include=['category', 'object']).columns\n",
    "    for col in nominal_columns:\n",
    "        if col == 'class':  # Skip the 'class' column\n",
    "            continue\n",
    "        unique_values = df[col].apply(lambda x: tuple(x) if isinstance(x, list) else x).unique()\n",
    "        indices = noisy_df.sample(frac=percentage / 100).index\n",
    "        noisy_df.loc[indices, col] = np.random.choice(unique_values, size=len(indices))\n",
    "    return noisy_df\n",
    "\n",
    "def add_noise_to_train(train_df, percentage, add_noise=True):\n",
    "    noisy_train = train_df.copy()\n",
    "    numeric_columns = train_df.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_columns:\n",
    "        noise = (train_df[col].max() - train_df[col].min()) * (percentage / 100)\n",
    "        if add_noise:\n",
    "            noisy_train[col] += noise\n",
    "        else:\n",
    "            noisy_train[col] -= noise\n",
    "    return noisy_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed0ea2e0-ea7e-4e33-8d02-2d70dbe6fd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_noisy_datasets(train_df, valid_df, num_variants=10):\n",
    "    noisy_datasets = []\n",
    "    for i in range(1, 11):\n",
    "        # Add noise\n",
    "        train_noisy_add = add_numeric_noise(train_df, i, add_noise=True)\n",
    "        valid_noisy_add = add_numeric_noise(valid_df, i, add_noise=True)\n",
    "        noisy_datasets.append((train_noisy_add, valid_noisy_add))\n",
    "        \n",
    "        # Subtract noise\n",
    "        train_noisy_subtract = add_numeric_noise(train_df, i, add_noise=False)\n",
    "        valid_noisy_subtract = add_numeric_noise(valid_df, i, add_noise=False)\n",
    "        noisy_datasets.append((train_noisy_subtract, valid_noisy_subtract))\n",
    "    return noisy_datasets\n",
    "\n",
    "def add_noise_to_train(train_df, percentage, add_noise=True):\n",
    "    noisy_train = train_df.copy()\n",
    "    numeric_columns = train_df.select_dtypes(include=[np.number]).columns\n",
    "    noise = (train_df[numeric_columns].max() - train_df[numeric_columns].min()) * (percentage / 100)\n",
    "    if add_noise:\n",
    "        noisy_train[numeric_columns] += noise\n",
    "    else:\n",
    "        noisy_train[numeric_columns] -= noise\n",
    "    return noisy_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62634248-496e-41ce-b5e9-268334234735",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'cellcycle'  # Replace with your dataset name\n",
    "dataset = Dataset(dataset_name, nan_strategy='mean')\n",
    "\n",
    "# # Load train and valid data\n",
    "# train_df = dataset.train\n",
    "# valid_df = dataset.valid\n",
    "# test_df = dataset.test\n",
    "\n",
    "# # # Generate and store noisy datasets in a list\n",
    "# # noisy_datasets = generate_noisy_datasets(train_df, valid_df, num_variants=10)\n",
    "\n",
    "# # # Display the first few rows of the first noisy train and valid dataset as an example\n",
    "# # noisy_train_1, noisy_valid_1 = noisy_datasets[1]\n",
    "# # noisy_train_1.head(), noisy_valid_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05da5e9c-1172-413d-8f91-90e574bdad40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b085054a-4ae7-41ef-b7c8-054cc7f563fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "def scale_data(df, method='standard'):\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if method == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    elif method == 'minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "    df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "    return df\n",
    "\n",
    "# Outliers handling options\n",
    "\n",
    "def remove_outliers_zscore(df, threshold=3.0):\n",
    "    z_scores = np.abs(stats.zscore(df.select_dtypes(include=[np.number])))\n",
    "    filtered_entries = (z_scores < threshold).all(axis=1)\n",
    "    return df[filtered_entries]\n",
    "\n",
    "def remove_outliers_iqr(df):\n",
    "    Q1 = df.quantile(0.25)\n",
    "    Q3 = df.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    filter = (df >= (Q1 - 1.5 * IQR)) & (df <= (Q3 + 1.5 * IQR))\n",
    "    return df[filter.all(axis=1)]\n",
    "\n",
    "# Imputing Outliers\n",
    "def clip_outliers(df, lower_percentile=0.01, upper_percentile=0.99):\n",
    "    lower_bound = df.quantile(lower_percentile)\n",
    "    upper_bound = df.quantile(upper_percentile)\n",
    "    return df.clip(lower=lower_bound, upper=upper_bound, axis=1)\n",
    "\n",
    "def replace_outliers_with_mean(df, threshold=3.0):\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        mean = df[col].mean()\n",
    "        std = df[col].std()\n",
    "        outliers = np.abs((df[col] - mean) / std) > threshold\n",
    "        df.loc[outliers, col] = mean\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a1e77d5-bd8e-427b-8784-2088ae5c36e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.4455458807769591}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from hiclass.MultiLabelLocalClassifierPerNode import MultiLabelLocalClassifierPerNode\n",
    "from hiclass.metrics import f1\n",
    "from feature_selection import fill_reshape\n",
    "\n",
    "def add_noise_to_train(train_df, percentage, add_noise=True):\n",
    "    noisy_train = train_df.copy()\n",
    "    numeric_columns = train_df.select_dtypes(include=[np.number]).columns\n",
    "    noise = (train_df[numeric_columns].max() - train_df[numeric_columns].min()) * (percentage / 100)\n",
    "    if add_noise:\n",
    "        noisy_train[numeric_columns] += noise\n",
    "    else:\n",
    "        noisy_train[numeric_columns] -= noise\n",
    "    return noisy_train\n",
    "\n",
    "class NoisePreprocessor(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, scaling_method='standard', outlier_method='zscore', imputation_method='mode', outlier_threshold=3.0):\n",
    "        self.scaling_method = scaling_method\n",
    "        self.outlier_method = outlier_method\n",
    "        self.imputation_method = imputation_method\n",
    "        self.outlier_threshold = outlier_threshold\n",
    "        self.scaler_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if self.outlier_method in ['zscore', 'iqr', 'clip', 'mean']:\n",
    "            X_clean, _ = self._remove_outliers(X, y)\n",
    "        else:\n",
    "            X_clean = X\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "\n",
    "        if self.outlier_method in ['zscore', 'iqr', 'clip', 'mean']:\n",
    "            X, y = self._remove_outliers(X, y)\n",
    "        \n",
    "        if self.imputation_method in ['mode', 'mean']:\n",
    "            X = self._impute_nominal_values(X)\n",
    "        \n",
    "        if y is not None:\n",
    "            return X, y\n",
    "        return X\n",
    "\n",
    "    def _remove_outliers(self, df, y=None):\n",
    "        if self.outlier_method == 'zscore':\n",
    "            return self._remove_outliers_zscore(df, y)\n",
    "        elif self.outlier_method == 'iqr':\n",
    "            return self._remove_outliers_iqr(df, y)\n",
    "        elif self.outlier_method == 'clip':\n",
    "            return self._clip_outliers(df, y)\n",
    "        elif self.outlier_method == 'mean':\n",
    "            return self._replace_outliers_with_mean(df, y)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown outlier method: {self.outlier_method}\")\n",
    "\n",
    "    def _remove_outliers_zscore(self, df, y=None):\n",
    "        numeric_cols = df.select_dtypes(include=[np.number])\n",
    "        z_scores = np.abs(stats.zscore(numeric_cols))\n",
    "        filtered_entries = (z_scores < self.outlier_threshold).all(axis=1)\n",
    "        if y is not None:\n",
    "            return df.loc[filtered_entries], y.loc[filtered_entries]\n",
    "        return df.loc[filtered_entries], y\n",
    "\n",
    "    def _remove_outliers_iqr(self, df, y=None):\n",
    "        Q1 = df.quantile(0.25)\n",
    "        Q3 = df.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        filter = (df >= (Q1 - 1.5 * IQR)) & (df <= (Q3 + 1.5 * IQR))\n",
    "        filtered_entries = filter.all(axis=1)\n",
    "        if y is not None:\n",
    "            return df.loc[filtered_entries], y.loc[filtered_entries]\n",
    "        return df.loc[filtered_entries], y\n",
    "\n",
    "    def _clip_outliers(self, df, y=None, lower_percentile=0.01, upper_percentile=0.99):\n",
    "        lower_bound = df.quantile(lower_percentile)\n",
    "        upper_bound = df.quantile(upper_percentile)\n",
    "        df_clipped = df.clip(lower=lower_bound, upper=upper_bound, axis=1)\n",
    "        return df_clipped, y\n",
    "\n",
    "    def _replace_outliers_with_mean(self, df, y=None):\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        for col in numeric_cols:\n",
    "            mean = df[col].mean()\n",
    "            std = df[col].std()\n",
    "            outliers = np.abs((df[col] - mean) / std) > self.outlier_threshold\n",
    "            df.loc[outliers, col] = mean\n",
    "        return df, y\n",
    "\n",
    "    def _scale_data(self, df):\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        df.loc[:, numeric_cols] = self.scaler_.transform(df[numeric_cols])\n",
    "        return df\n",
    "\n",
    "    def _impute_nominal_values(self, df):\n",
    "        nominal_cols = df.select_dtypes(include=['category', 'object']).columns\n",
    "        for col in nominal_cols:\n",
    "            if self.imputation_method == 'mode':\n",
    "                mode_value = df[col].mode()[0]\n",
    "                df[col].fillna(mode_value, inplace=True)\n",
    "            elif self.imputation_method == 'mean' and df[col].dtype == np.number:\n",
    "                mean_value = df[col].mean()\n",
    "                df[col].fillna(mean_value, inplace=True)\n",
    "        return df\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', NoisePreprocessor(\n",
    "        scaling_method=None, \n",
    "        outlier_method='mean', \n",
    "        imputation_method=None, \n",
    "        outlier_threshold=3.0\n",
    "    )),\n",
    "    ('classifier', MultiLabelLocalClassifierPerNode(\n",
    "        local_classifier=LogisticRegression(\n",
    "            penalty='l2',\n",
    "            C=0.01,\n",
    "            solver='lbfgs',\n",
    "            max_iter=10000\n",
    "        )\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Introducing noise to x train\n",
    "noisy_train_x = add_noise_to_train(dataset.x_train(), percentage=10)\n",
    "\n",
    "pipeline.fit(noisy_train_x, dataset.y_train())\n",
    "\n",
    "y_pred_test = pipeline.predict(dataset.x_test())\n",
    "test_accuracy = f1(fill_reshape(dataset.y_test()), fill_reshape(y_pred_test))\n",
    "\n",
    "results = {\n",
    "    'f1': test_accuracy\n",
    "}\n",
    "\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
