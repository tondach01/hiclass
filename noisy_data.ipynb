{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69ec9bec-43c3-4b6d-a1a6-b68ce50ac711",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from load import Dataset\n",
    "\n",
    "def add_nominal_noise(df, percentage):\n",
    "    noisy_df = df.copy()\n",
    "    nominal_columns = df.select_dtypes(include=['category', 'object']).columns\n",
    "    for col in nominal_columns:\n",
    "        if col == 'class':  # Skip the 'class' column\n",
    "            continue\n",
    "        unique_values = df[col].apply(lambda x: tuple(x) if isinstance(x, list) else x).unique()\n",
    "        indices = noisy_df.sample(frac=percentage / 100).index\n",
    "        noisy_df.loc[indices, col] = np.random.choice(unique_values, size=len(indices))\n",
    "    return noisy_df\n",
    "\n",
    "def add_numeric_noise(df, percentage, add_noise=True):\n",
    "    noisy_df = df.copy()\n",
    "    numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_columns:\n",
    "        noise = (df[col].max() - df[col].min()) * (percentage / 100)\n",
    "        if add_noise:\n",
    "            noisy_df[col] = df[col] + noise\n",
    "        else:\n",
    "            noisy_df[col] = df[col] - noise\n",
    "    return noisy_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed0ea2e0-ea7e-4e33-8d02-2d70dbe6fd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_noisy_datasets(train_df, valid_df, num_variants=10):\n",
    "    noisy_datasets = []\n",
    "    for i in range(1, 11):\n",
    "        # Add noise\n",
    "        train_noisy_add = add_numeric_noise(train_df, i, add_noise=True)\n",
    "        valid_noisy_add = add_numeric_noise(valid_df, i, add_noise=True)\n",
    "        noisy_datasets.append((train_noisy_add, valid_noisy_add))\n",
    "        \n",
    "        # Subtract noise\n",
    "        train_noisy_subtract = add_numeric_noise(train_df, i, add_noise=False)\n",
    "        valid_noisy_subtract = add_numeric_noise(valid_df, i, add_noise=False)\n",
    "        noisy_datasets.append((train_noisy_subtract, valid_noisy_subtract))\n",
    "    return noisy_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62634248-496e-41ce-b5e9-268334234735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   cln3-1    cln3-2  clb2-2  clb2-1  alpha0  alpha7  alpha14  alpha21  \\\n",
       " 0  0.0733 -0.282872 -0.2916 -0.0096  -0.222 -0.2178  -0.2491   0.1301   \n",
       " 1 -1.2967 -0.333100 -0.1716  0.1504  -0.212 -0.7778   0.0609  -0.3599   \n",
       " 2 -0.6767  0.946900  0.1684  0.5704  -0.122 -0.5978  -0.5091  -0.0999   \n",
       " 3  0.1733 -0.853100 -0.2916 -0.6196  -0.102 -0.3378   0.1309  -0.1599   \n",
       " 4 -0.1967 -0.603100 -0.1916 -0.2596  -0.072 -0.0778   0.0809  -0.2699   \n",
       " \n",
       "    alpha28  alpha35  ...  elu150  elu180  elu210  elu240  elu270  elu300  \\\n",
       " 0  -0.4498  -0.4725  ...   0.083 -0.1454  0.3454  0.0432 -0.1176 -0.3589   \n",
       " 1  -0.4298  -0.6125  ...   0.253  0.1246  0.3754  0.2132  0.1924 -0.3089   \n",
       " 2   0.0802  -0.1025  ...  -0.857 -0.2854 -0.1846  0.2732  0.1824 -0.4689   \n",
       " 3  -0.2998   0.0275  ...   0.093  0.2646 -0.1246 -0.4368 -0.6776 -0.1089   \n",
       " 4  -0.1598   0.2175  ...  -0.427 -0.2054  0.0954  0.3032 -0.0576 -0.1989   \n",
       " \n",
       "    elu330  elu360  elu390                                              class  \n",
       " 0  0.0054 -0.5065  0.0131                  [11/02/01, 11/02/02, 11/02/03/01]  \n",
       " 1 -0.1346  0.3135 -0.0069                                         [12/04/02]  \n",
       " 2  0.1754  0.5735  0.6231  [01/04, 14/01, 14/04, 16/01, 16/19/03, 20/09/0...  \n",
       " 3 -0.3846 -0.2165  0.1231                     [10/03/02, 42/10, 43/01/03/09]  \n",
       " 4  0.2954  0.2535 -0.0169                               [10/01/05/01, 32/01]  \n",
       " \n",
       " [5 rows x 78 columns],\n",
       "    cln3-1  cln3-2  clb2-2  clb2-1  alpha0  alpha7  alpha14  alpha21  alpha28  \\\n",
       " 0 -0.1459 -0.8344 -0.1945 -0.3072 -0.1548    0.05   -0.023   0.0211   0.0113   \n",
       " 1 -0.3759 -0.6744 -0.2545 -0.0472 -0.0948    0.08    0.097  -0.2489  -0.4787   \n",
       " 2 -1.2959  0.5856 -0.7145 -0.2272 -0.0248   -0.38   -0.523  -0.3389  -0.1787   \n",
       " 3  0.3641 -0.5844 -0.2945  0.0928 -0.0648   -0.34   -0.103  -0.2589  -0.0887   \n",
       " 4  0.9041  1.2556 -0.3445 -0.3472 -0.6148   -1.17   -0.243   0.0611   0.6013   \n",
       " \n",
       "    alpha35  ...  elu150  elu180  elu210  elu240  elu270  elu300  elu330  \\\n",
       " 0  -0.2942  ... -0.1057 -0.1518 -0.2306 -0.0816 -0.1242 -0.2145  0.0596   \n",
       " 1  -0.2442  ... -0.1957  0.3482 -0.1006 -0.0316 -0.0042  0.3155  0.0296   \n",
       " 2  -0.2742  ...  0.1443  0.2482  0.0094  0.0684  0.4458 -0.1045  0.0896   \n",
       " 3  -0.1042  ... -0.2257  0.2482  0.2694 -0.3616 -0.0742  0.2055  0.0596   \n",
       " 4   0.3758  ...  0.1443  0.4682  0.6194  0.5084  0.5558 -0.0545  0.4696   \n",
       " \n",
       "    elu360  elu390                                class  \n",
       " 0  -0.017  -0.037                    [14/04, 20/09/13]  \n",
       " 1  -0.957   0.033                              [42/16]  \n",
       " 2  -0.437   0.143  [01/01/06/05/01/01, 01/01/09/03/01]  \n",
       " 3  -0.477   0.233       [10/03/05/01, 42/04/05, 42/29]  \n",
       " 4   0.013  -0.047                 [01/05, 14/07/02/01]  \n",
       " \n",
       " [5 rows x 78 columns])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = 'cellcycle'  # Replace with your dataset name\n",
    "dataset = Dataset(dataset_name, nan_strategy='mean')\n",
    "\n",
    "# Load train and valid data\n",
    "train_df = dataset.train\n",
    "valid_df = dataset.valid\n",
    "test_df = dataset.test\n",
    "\n",
    "# Generate and store noisy datasets in a list\n",
    "noisy_datasets = generate_noisy_datasets(train_df, valid_df, num_variants=10)\n",
    "\n",
    "# Display the first few rows of the first noisy train and valid dataset as an example\n",
    "noisy_train_1, noisy_valid_1 = noisy_datasets[1]\n",
    "noisy_train_1.head(), noisy_valid_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05da5e9c-1172-413d-8f91-90e574bdad40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cln3-1</th>\n",
       "      <th>cln3-2</th>\n",
       "      <th>clb2-2</th>\n",
       "      <th>clb2-1</th>\n",
       "      <th>alpha0</th>\n",
       "      <th>alpha7</th>\n",
       "      <th>alpha14</th>\n",
       "      <th>alpha21</th>\n",
       "      <th>alpha28</th>\n",
       "      <th>alpha35</th>\n",
       "      <th>...</th>\n",
       "      <th>elu150</th>\n",
       "      <th>elu180</th>\n",
       "      <th>elu210</th>\n",
       "      <th>elu240</th>\n",
       "      <th>elu270</th>\n",
       "      <th>elu300</th>\n",
       "      <th>elu330</th>\n",
       "      <th>elu360</th>\n",
       "      <th>elu390</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.219772</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>...</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>0.04</td>\n",
       "      <td>[11/02/01, 11/02/02, 11/02/03/01]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.22</td>\n",
       "      <td>-0.270000</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.23</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.71</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.58</td>\n",
       "      <td>...</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.22</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.02</td>\n",
       "      <td>[12/04/02]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.60</td>\n",
       "      <td>1.010000</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.65</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.53</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.83</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.21</td>\n",
       "      <td>-0.43</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.65</td>\n",
       "      <td>[01/04, 14/01, 14/04, 16/01, 16/19/03, 20/09/0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.25</td>\n",
       "      <td>-0.790000</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>-0.54</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>0.06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>-0.65</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>0.15</td>\n",
       "      <td>[10/03/02, 42/10, 43/01/03/09]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.540000</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.33</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[10/01/05/01, 32/01]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cln3-1    cln3-2  clb2-2  clb2-1  alpha0  alpha7  alpha14  alpha21  \\\n",
       "0    0.15 -0.219772   -0.22    0.07   -0.15   -0.15    -0.21     0.17   \n",
       "1   -1.22 -0.270000   -0.10    0.23   -0.14   -0.71     0.10    -0.32   \n",
       "2   -0.60  1.010000    0.24    0.65   -0.05   -0.53    -0.47    -0.06   \n",
       "3    0.25 -0.790000   -0.22   -0.54   -0.03   -0.27     0.17    -0.12   \n",
       "4   -0.12 -0.540000   -0.12   -0.18    0.00   -0.01     0.12    -0.23   \n",
       "\n",
       "   alpha28  alpha35  ...  elu150  elu180  elu210  elu240  elu270  elu300  \\\n",
       "0    -0.42    -0.44  ...    0.11   -0.12    0.37    0.07   -0.09   -0.32   \n",
       "1    -0.40    -0.58  ...    0.28    0.15    0.40    0.24    0.22   -0.27   \n",
       "2     0.11    -0.07  ...   -0.83   -0.26   -0.16    0.30    0.21   -0.43   \n",
       "3    -0.27     0.06  ...    0.12    0.29   -0.10   -0.41   -0.65   -0.07   \n",
       "4    -0.13     0.25  ...   -0.40   -0.18    0.12    0.33   -0.03   -0.16   \n",
       "\n",
       "   elu330  elu360  elu390                                              class  \n",
       "0    0.04   -0.48    0.04                  [11/02/01, 11/02/02, 11/02/03/01]  \n",
       "1   -0.10    0.34    0.02                                         [12/04/02]  \n",
       "2    0.21    0.60    0.65  [01/04, 14/01, 14/04, 16/01, 16/19/03, 20/09/0...  \n",
       "3   -0.35   -0.19    0.15                     [10/03/02, 42/10, 43/01/03/09]  \n",
       "4    0.33    0.28    0.01                               [10/01/05/01, 32/01]  \n",
       "\n",
       "[5 rows x 78 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b085054a-4ae7-41ef-b7c8-054cc7f563fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "def scale_data(df, method='standard'):\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if method == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    elif method == 'minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "    df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "    return df\n",
    "\n",
    "# Outliers handling options\n",
    "\n",
    "def remove_outliers_zscore(df, threshold=3.0):\n",
    "    z_scores = np.abs(stats.zscore(df.select_dtypes(include=[np.number])))\n",
    "    filtered_entries = (z_scores < threshold).all(axis=1)\n",
    "    return df[filtered_entries]\n",
    "\n",
    "def remove_outliers_iqr(df):\n",
    "    Q1 = df.quantile(0.25)\n",
    "    Q3 = df.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    filter = (df >= (Q1 - 1.5 * IQR)) & (df <= (Q3 + 1.5 * IQR))\n",
    "    return df[filter.all(axis=1)]\n",
    "\n",
    "# Imputing Outliers\n",
    "def clip_outliers(df, lower_percentile=0.01, upper_percentile=0.99):\n",
    "    lower_bound = df.quantile(lower_percentile)\n",
    "    upper_bound = df.quantile(upper_percentile)\n",
    "    return df.clip(lower=lower_bound, upper=upper_bound, axis=1)\n",
    "\n",
    "def replace_outliers_with_mean(df, threshold=3.0):\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        mean = df[col].mean()\n",
    "        std = df[col].std()\n",
    "        outliers = np.abs((df[col] - mean) / std) > threshold\n",
    "        df.loc[outliers, col] = mean\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8922ae9-7180-48d1-896e-16895f292e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-26 15:36:37,868 - LCPN - INFO - Creating digraph from 1628 3D labels\n",
      "2024-05-26 15:36:39,330 - LCPN - INFO - Detected 6 roots\n",
      "2024-05-26 15:36:39,331 - LCPN - INFO - Initializing local classifiers\n",
      "2024-05-26 15:36:39,343 - LCPN - INFO - Initializing siblings binary policy\n",
      "2024-05-26 15:36:39,343 - LCPN - INFO - Fitting local classifiers\n",
      "2024-05-26 15:41:02,078 - LCPN - INFO - Cleaning up variables that can take a lot of disk space\n",
      "2024-05-26 15:41:02,391 - LCPN - INFO - Predicting\n",
      "2024-05-26 15:41:02,402 - LCPN - INFO - Predicting for node '1'\n",
      "2024-05-26 15:41:02,414 - LCPN - INFO - Predicting for node '0'\n",
      "2024-05-26 15:41:02,416 - LCPN - INFO - Predicting for node '2'\n",
      "2024-05-26 15:41:02,418 - LCPN - INFO - Predicting for node '3'\n",
      "2024-05-26 15:41:02,421 - LCPN - INFO - Predicting for node '4'\n",
      "2024-05-26 15:41:02,423 - LCPN - INFO - Predicting for node '9'\n",
      "2024-05-26 15:41:02,435 - LCPN - INFO - Predicting for node '1'\n",
      "2024-05-26 15:41:02,435 - LCPN - INFO - Predicting for node '2'\n",
      "2024-05-26 15:41:02,435 - LCPN - INFO - Predicting for node '4'\n",
      "2024-05-26 15:41:02,443 - LCPN - INFO - Predicting for node '6'\n",
      "2024-05-26 15:41:02,446 - LCPN - INFO - Predicting for node '0'\n",
      "2024-05-26 15:41:02,450 - LCPN - INFO - Predicting for node '8'\n",
      "2024-05-26 15:41:02,455 - LCPN - INFO - Predicting for node '1'\n",
      "2024-05-26 15:41:02,457 - LCPN - INFO - Predicting for node '2'\n",
      "2024-05-26 15:41:02,466 - LCPN - INFO - Predicting for node '/'\n",
      "2024-05-26 15:41:02,466 - LCPN - INFO - Predicting for node '/'\n",
      "2024-05-26 15:41:02,478 - LCPN - INFO - Predicting for node '/'\n",
      "2024-05-26 15:41:02,485 - LCPN - INFO - Predicting for node '/'\n",
      "2024-05-26 15:41:02,491 - LCPN - INFO - Predicting for node '/'\n",
      "2024-05-26 15:41:02,498 - LCPN - INFO - Predicting for node '/'\n",
      "2024-05-26 15:41:02,519 - LCPN - INFO - Predicting for node '0'\n",
      "2024-05-26 15:41:02,526 - LCPN - INFO - Predicting for node '0'\n",
      "2024-05-26 15:41:02,528 - LCPN - INFO - Predicting for node '1'\n",
      "2024-05-26 15:41:02,528 - LCPN - INFO - Predicting for node '0'\n",
      "2024-05-26 15:41:02,528 - LCPN - INFO - Predicting for node '1'\n",
      "2024-05-26 15:41:02,541 - LCPN - INFO - Predicting for node '0'\n",
      "2024-05-26 15:41:02,544 - LCPN - INFO - Predicting for node '1'\n",
      "2024-05-26 15:41:02,547 - LCPN - INFO - Predicting for node '2'\n",
      "2024-05-26 15:41:02,547 - LCPN - INFO - Predicting for node '0'\n",
      "2024-05-26 15:41:02,559 - LCPN - INFO - Predicting for node '0'\n",
      "2024-05-26 15:41:02,562 - LCPN - INFO - Predicting for node '2'\n",
      "2024-05-26 15:41:02,583 - LCPN - INFO - Predicting for node '2'\n",
      "2024-05-26 15:41:02,585 - LCPN - INFO - Predicting for node '4'\n",
      "2024-05-26 15:41:02,588 - LCPN - INFO - Predicting for node '6'\n",
      "2024-05-26 15:41:02,594 - LCPN - INFO - Predicting for node '4'\n",
      "2024-05-26 15:41:02,598 - LCPN - INFO - Predicting for node '1'\n",
      "2024-05-26 15:41:02,600 - LCPN - INFO - Predicting for node '7'\n",
      "2024-05-26 15:41:02,606 - LCPN - INFO - Predicting for node '1'\n",
      "2024-05-26 15:41:02,608 - LCPN - INFO - Predicting for node '4'\n",
      "2024-05-26 15:41:02,613 - LCPN - INFO - Predicting for node '7'\n",
      "2024-05-26 15:41:02,619 - LCPN - INFO - Predicting for node '1'\n",
      "2024-05-26 15:41:02,620 - LCPN - INFO - Predicting for node '3'\n",
      "2024-05-26 15:41:02,622 - LCPN - INFO - Predicting for node '2'\n",
      "2024-05-26 15:41:02,626 - LCPN - INFO - Predicting for node '9'\n",
      "2024-05-26 15:41:02,628 - LCPN - INFO - Predicting for node '7'\n",
      "2024-05-26 15:41:02,631 - LCPN - INFO - Predicting for node '6'\n",
      "2024-05-26 15:41:02,638 - LCPN - INFO - Predicting for node '3'\n",
      "2024-05-26 15:41:02,644 - LCPN - INFO - Predicting for node '1'\n",
      "2024-05-26 15:41:02,650 - LCPN - INFO - Predicting for node '4'\n",
      "2024-05-26 15:41:02,652 - LCPN - INFO - Predicting for node '7'\n",
      "2024-05-26 15:41:02,653 - LCPN - INFO - Predicting for node '5'\n",
      "2024-05-26 15:41:02,655 - LCPN - INFO - Predicting for node '1'\n",
      "2024-05-26 15:41:02,658 - LCPN - INFO - Predicting for node '2'\n",
      "2024-05-26 15:41:02,663 - LCPN - INFO - Predicting for node '3'\n",
      "2024-05-26 15:41:02,666 - LCPN - INFO - Predicting for node '6'\n",
      "2024-05-26 15:41:02,704 - LCPN - INFO - Predicting for node '/'\n",
      "2024-05-26 15:41:02,713 - LCPN - INFO - Predicting for node '/'\n",
      "2024-05-26 15:41:02,720 - LCPN - INFO - Predicting for node '/'\n",
      "2024-05-26 15:41:02,729 - LCPN - INFO - Predicting for node '/'\n",
      "2024-05-26 15:41:02,739 - LCPN - INFO - Predicting for node '/'\n",
      "2024-05-26 15:41:02,749 - LCPN - INFO - Predicting for node '/'\n",
      "2024-05-26 15:41:02,753 - LCPN - INFO - Predicting for node '/'\n",
      "2024-05-26 15:41:02,764 - LCPN - INFO - Predicting for node '/'\n",
      "2024-05-26 15:41:02,764 - LCPN - INFO - Predicting for node '/'\n",
      "2024-05-26 15:41:02,815 - LCPN - INFO - Predicting for node '0'\n",
      "2024-05-26 15:41:02,818 - LCPN - INFO - Predicting for node '0'\n",
      "2024-05-26 15:41:02,825 - LCPN - INFO - Predicting for node '0'\n",
      "2024-05-26 15:41:02,828 - LCPN - INFO - Predicting for node '0'\n",
      "2024-05-26 15:41:02,831 - LCPN - INFO - Predicting for node '1'\n",
      "2024-05-26 15:41:02,838 - LCPN - INFO - Predicting for node '0'\n",
      "2024-05-26 15:41:02,849 - LCPN - INFO - Predicting for node '0'\n",
      "2024-05-26 15:41:02,850 - LCPN - INFO - Predicting for node '0'\n",
      "2024-05-26 15:41:02,858 - LCPN - INFO - Predicting for node '1'\n",
      "2024-05-26 15:41:02,867 - LCPN - INFO - Predicting for node '0'\n",
      "2024-05-26 15:41:02,869 - LCPN - INFO - Predicting for node '1'\n",
      "2024-05-26 15:41:02,869 - LCPN - INFO - Predicting for node '2'\n",
      "2024-05-26 15:41:02,869 - LCPN - INFO - Predicting for node '0'\n",
      "2024-05-26 15:41:02,876 - LCPN - INFO - Predicting for node '1'\n",
      "2024-05-26 15:41:02,919 - LCPN - INFO - Predicting for node '1'\n",
      "2024-05-26 15:41:02,919 - LCPN - INFO - Predicting for node '2'\n",
      "2024-05-26 15:41:02,919 - LCPN - INFO - Predicting for node '3'\n",
      "2024-05-26 15:41:02,928 - LCPN - INFO - Predicting for node '1'\n",
      "2024-05-26 15:41:02,929 - LCPN - INFO - Predicting for node '2'\n",
      "2024-05-26 15:41:02,930 - LCPN - INFO - Predicting for node '3'\n",
      "2024-05-26 15:41:02,935 - LCPN - INFO - Predicting for node '1'\n",
      "2024-05-26 15:41:02,938 - LCPN - INFO - Predicting for node '3'\n",
      "2024-05-26 15:41:02,940 - LCPN - INFO - Predicting for node '5'\n",
      "2024-05-26 15:41:02,942 - LCPN - INFO - Predicting for node '2'\n",
      "2024-05-26 15:41:02,944 - LCPN - INFO - Predicting for node '4'\n",
      "2024-05-26 15:41:02,948 - LCPN - INFO - Predicting for node '7'\n",
      "2024-05-26 15:41:02,950 - LCPN - INFO - Predicting for node '9'\n",
      "2024-05-26 15:41:02,952 - LCPN - INFO - Predicting for node '1'\n",
      "2024-05-26 15:41:02,962 - LCPN - INFO - Predicting for node '3'\n",
      "2024-05-26 15:41:02,965 - LCPN - INFO - Predicting for node '1'\n",
      "2024-05-26 15:41:02,972 - LCPN - INFO - Predicting for node '2'\n",
      "2024-05-26 15:41:02,974 - LCPN - INFO - Predicting for node '1'\n",
      "2024-05-26 15:41:02,976 - LCPN - INFO - Predicting for node '4'\n",
      "2024-05-26 15:41:02,978 - LCPN - INFO - Predicting for node '3'\n",
      "2024-05-26 15:41:02,980 - LCPN - INFO - Predicting for node '5'\n",
      "2024-05-26 15:41:02,980 - LCPN - INFO - Predicting for node '5'\n",
      "2024-05-26 15:41:02,980 - LCPN - INFO - Predicting for node '9'\n",
      "2024-05-26 15:41:02,980 - LCPN - INFO - Predicting for node '2'\n",
      "2024-05-26 15:41:02,980 - LCPN - INFO - Predicting for node '3'\n",
      "2024-05-26 15:41:03,001 - LCPN - INFO - Predicting for node '6'\n",
      "2024-05-26 15:41:03,002 - LCPN - INFO - Predicting for node '3'\n",
      "2024-05-26 15:41:03,004 - LCPN - INFO - Predicting for node '2'\n",
      "2024-05-26 15:41:03,006 - LCPN - INFO - Predicting for node '5'\n",
      "2024-05-26 15:41:03,007 - LCPN - INFO - Predicting for node '9'\n",
      "2024-05-26 15:41:03,015 - LCPN - INFO - Predicting for node '3'\n",
      "2024-05-26 15:41:03,017 - LCPN - INFO - Predicting for node '6'\n",
      "2024-05-26 15:41:03,018 - LCPN - INFO - Predicting for node '9'\n",
      "2024-05-26 15:41:03,019 - LCPN - INFO - Predicting for node '5'\n",
      "2024-05-26 15:41:03,065 - LCPN - INFO - Predicting for node '/'\n",
      "2024-05-26 15:41:03,081 - LCPN - INFO - Predicting for node '/'\n",
      "2024-05-26 15:41:03,090 - LCPN - INFO - Predicting for node '/'\n",
      "2024-05-26 15:41:03,097 - LCPN - INFO - Predicting for node '/'\n",
      "2024-05-26 15:41:03,108 - LCPN - INFO - Predicting for node '/'\n",
      "2024-05-26 15:41:03,125 - LCPN - INFO - Predicting for node '/'\n",
      "2024-05-26 15:41:03,196 - LCPN - INFO - Predicting for node '0'\n",
      "2024-05-26 15:41:03,213 - LCPN - INFO - Predicting for node '0'\n",
      "2024-05-26 15:41:03,221 - LCPN - INFO - Predicting for node '0'\n",
      "2024-05-26 15:41:03,231 - LCPN - INFO - Predicting for node '0'\n",
      "2024-05-26 15:41:03,235 - LCPN - INFO - Predicting for node '0'\n",
      "2024-05-26 15:41:03,245 - LCPN - INFO - Predicting for node '0'\n",
      "2024-05-26 15:41:03,296 - LCPN - INFO - Predicting for node '1'\n",
      "2024-05-26 15:41:03,298 - LCPN - INFO - Predicting for node '4'\n",
      "2024-05-26 15:41:03,316 - LCPN - INFO - Predicting for node '1'\n",
      "2024-05-26 15:41:03,317 - LCPN - INFO - Predicting for node '2'\n",
      "2024-05-26 15:41:03,319 - LCPN - INFO - Predicting for node '3'\n",
      "2024-05-26 15:41:03,328 - LCPN - INFO - Predicting for node '1'\n",
      "2024-05-26 15:41:03,330 - LCPN - INFO - Predicting for node '3'\n",
      "2024-05-26 15:41:03,339 - LCPN - INFO - Predicting for node '3'\n",
      "2024-05-26 15:41:03,342 - LCPN - INFO - Predicting for node '2'\n",
      "2024-05-26 15:41:03,345 - LCPN - INFO - Predicting for node '1'\n",
      "2024-05-26 15:41:03,349 - LCPN - INFO - Predicting for node '4'\n",
      "2024-05-26 15:41:03,350 - LCPN - INFO - Predicting for node '7'\n",
      "2024-05-26 15:41:03,362 - LCPN - INFO - Predicting for node '6'\n",
      "2024-05-26 15:41:03,363 - LCPN - INFO - Predicting for node '7'\n",
      "2024-05-26 15:41:03,364 - LCPN - INFO - Predicting for node '3'\n",
      "2024-05-26 15:41:03,365 - LCPN - INFO - Predicting for node '2'\n",
      "2024-05-26 15:41:03,367 - LCPN - INFO - Predicting for node '4'\n",
      "2024-05-26 15:41:03,368 - LCPN - INFO - Predicting for node '5'\n",
      "2024-05-26 15:41:03,372 - LCPN - INFO - Predicting for node '1'\n",
      "2024-05-26 15:41:03,998 - LCPN - INFO - Predicting for node '/'\n",
      "2024-05-26 15:41:04,005 - LCPN - INFO - Predicting for node '/'\n",
      "2024-05-26 15:41:04,016 - LCPN - INFO - Predicting for node '/'\n",
      "2024-05-26 15:41:04,028 - LCPN - INFO - Predicting for node '/'\n",
      "2024-05-26 15:41:04,051 - LCPN - INFO - Predicting for node '0'\n",
      "2024-05-26 15:41:04,055 - LCPN - INFO - Predicting for node '1'\n",
      "2024-05-26 15:41:04,058 - LCPN - INFO - Predicting for node '0'\n",
      "2024-05-26 15:41:04,065 - LCPN - INFO - Predicting for node '0'\n",
      "2024-05-26 15:41:04,084 - LCPN - INFO - Predicting for node '0'\n",
      "2024-05-26 15:41:04,115 - LCPN - INFO - Predicting for node '1'\n",
      "2024-05-26 15:41:04,116 - LCPN - INFO - Predicting for node '3'\n",
      "2024-05-26 15:41:04,125 - LCPN - INFO - Predicting for node '1'\n",
      "2024-05-26 15:41:04,127 - LCPN - INFO - Predicting for node '5'\n",
      "2024-05-26 15:41:04,128 - LCPN - INFO - Predicting for node '9'\n",
      "2024-05-26 15:41:04,131 - LCPN - INFO - Predicting for node '3'\n",
      "2024-05-26 15:41:04,142 - LCPN - INFO - Predicting for node '4'\n",
      "2024-05-26 15:41:04,153 - LCPN - INFO - Predicting for node '1'\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You appear to be using a legacy multi-label data representation. Sequence of sequences are no longer supported; use a binary array or sparse matrix instead - the MultiLabelBinarizer transformer can convert to this format.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 34\u001b[0m\n\u001b[0;32m     25\u001b[0m     results \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     26\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: valid_accuracy,\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: test_accuracy,\n\u001b[0;32m     28\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid_classification_report\u001b[39m\u001b[38;5;124m'\u001b[39m: classification_report(y_valid, y_pred_valid),\n\u001b[0;32m     29\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_classification_report\u001b[39m\u001b[38;5;124m'\u001b[39m: classification_report(y_test, y_pred_test),\n\u001b[0;32m     30\u001b[0m     }\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m---> 34\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregularization_strength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(results)\n",
      "Cell \u001b[1;32mIn[13], line 20\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(train_df, valid_df, test_df, regularization_strength)\u001b[0m\n\u001b[0;32m     17\u001b[0m classifier\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     19\u001b[0m y_pred_valid \u001b[38;5;241m=\u001b[39m classifier\u001b[38;5;241m.\u001b[39mpredict(X_valid)\n\u001b[1;32m---> 20\u001b[0m valid_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred_valid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m y_pred_test \u001b[38;5;241m=\u001b[39m classifier\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     23\u001b[0m test_accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_test, y_pred_test)\n",
      "File \u001b[1;32m~\\Desktop\\hiclass_repo\\hiclass\\venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32m~\\Desktop\\hiclass_repo\\hiclass\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:213\u001b[0m, in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \n\u001b[0;32m    149\u001b[0m \u001b[38;5;124;03mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;124;03m0.5\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[1;32m--> 213\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\Desktop\\hiclass_repo\\hiclass\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:86\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03mThis converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03my_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     85\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[1;32m---> 86\u001b[0m type_true \u001b[38;5;241m=\u001b[39m \u001b[43mtype_of_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my_true\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     89\u001b[0m y_type \u001b[38;5;241m=\u001b[39m {type_true, type_pred}\n",
      "File \u001b[1;32m~\\Desktop\\hiclass_repo\\hiclass\\venv\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:353\u001b[0m, in \u001b[0;36mtype_of_target\u001b[1;34m(y, input_name)\u001b[0m\n\u001b[0;32m    347\u001b[0m     first_row \u001b[38;5;241m=\u001b[39m y[[\u001b[38;5;241m0\u001b[39m], :] \u001b[38;5;28;01mif\u001b[39;00m issparse(y) \u001b[38;5;28;01melse\u001b[39;00m y[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    349\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(first_row, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__array__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    350\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(first_row, Sequence)\n\u001b[0;32m    351\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(first_row, \u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m    352\u001b[0m     ):\n\u001b[1;32m--> 353\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    354\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou appear to be using a legacy multi-label data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    355\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m representation. Sequence of sequences are no\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    356\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m longer supported; use a binary array or sparse\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    357\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m matrix instead - the MultiLabelBinarizer\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    358\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m transformer can convert to this format.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    359\u001b[0m         )\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n\u001b[0;32m    361\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: You appear to be using a legacy multi-label data representation. Sequence of sequences are no longer supported; use a binary array or sparse matrix instead - the MultiLabelBinarizer transformer can convert to this format."
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from hiclass.MultiLabelLocalClassifierPerNode import MultiLabelLocalClassifierPerNode\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "def train_and_evaluate(train_df, valid_df, test_df, regularization_strength=0.01):\n",
    "    X_train, y_train = train_df.drop(columns=['class']), train_df['class']\n",
    "    X_valid, y_valid = valid_df.drop(columns=['class']), valid_df['class']\n",
    "    X_test, y_test = test_df.drop(columns=['class']), test_df['class']\n",
    "    \n",
    "    # L2 reg\n",
    "    classifier = MultiLabelLocalClassifierPerNode(\n",
    "        local_classifier=LogisticRegression(penalty='l2', C=regularization_strength, solver='lbfgs', max_iter=10000),\n",
    "        tolerance=0.001,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_valid = classifier.predict(X_valid)\n",
    "    valid_accuracy = accuracy_score(y_valid, y_pred_valid)\n",
    "    \n",
    "    y_pred_test = classifier.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "    \n",
    "    results = {\n",
    "        'valid_accuracy': valid_accuracy,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'valid_classification_report': classification_report(y_valid, y_pred_valid),\n",
    "        'test_classification_report': classification_report(y_test, y_pred_test),\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "results = train_and_evaluate(train_df, valid_df, test_df, regularization_strength=0.01)\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
