{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69ec9bec-43c3-4b6d-a1a6-b68ce50ac711",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from load import Dataset\n",
    "\n",
    "def add_nominal_noise(df, percentage):\n",
    "    noisy_df = df.copy()\n",
    "    nominal_columns = df.select_dtypes(include=['category', 'object']).columns\n",
    "    for col in nominal_columns:\n",
    "        if col == 'class':  # Skip the 'class' column\n",
    "            continue\n",
    "        unique_values = df[col].apply(lambda x: tuple(x) if isinstance(x, list) else x).unique()\n",
    "        indices = noisy_df.sample(frac=percentage / 100).index\n",
    "        noisy_df.loc[indices, col] = np.random.choice(unique_values, size=len(indices))\n",
    "    return noisy_df\n",
    "\n",
    "def add_numeric_noise(df, percentage, add_noise=True):\n",
    "    noisy_df = df.copy()\n",
    "    numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_columns:\n",
    "        noise = (df[col].max() - df[col].min()) * (percentage / 100)\n",
    "        if add_noise:\n",
    "            noisy_df[col] = df[col] + noise\n",
    "        else:\n",
    "            noisy_df[col] = df[col] - noise\n",
    "    return noisy_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed0ea2e0-ea7e-4e33-8d02-2d70dbe6fd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_noisy_datasets(train_df, valid_df, num_variants=10):\n",
    "    noisy_datasets = []\n",
    "    for i in range(1, 11):\n",
    "        # Add noise\n",
    "        train_noisy_add = add_numeric_noise(train_df, i, add_noise=True)\n",
    "        valid_noisy_add = add_numeric_noise(valid_df, i, add_noise=True)\n",
    "        noisy_datasets.append((train_noisy_add, valid_noisy_add))\n",
    "        \n",
    "        # Subtract noise\n",
    "        train_noisy_subtract = add_numeric_noise(train_df, i, add_noise=False)\n",
    "        valid_noisy_subtract = add_numeric_noise(valid_df, i, add_noise=False)\n",
    "        noisy_datasets.append((train_noisy_subtract, valid_noisy_subtract))\n",
    "    return noisy_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62634248-496e-41ce-b5e9-268334234735",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Index dimension must be 1 or 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m dataset_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcellcycle\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Replace with your dataset name\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnan_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load train and valid data\u001b[39;00m\n\u001b[0;32m      5\u001b[0m train_df \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mtrain\n",
      "File \u001b[1;32m~\\Desktop\\hiclass_repo\\hiclass\\load.py:83\u001b[0m, in \u001b[0;36mDataset.__init__\u001b[1;34m(self, dataset_name, nan_strategy, args)\u001b[0m\n\u001b[0;32m     80\u001b[0m     remove(file)\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[1;32m---> 83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain \u001b[38;5;241m=\u001b[39m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest \u001b[38;5;241m=\u001b[39m _read(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid \u001b[38;5;241m=\u001b[39m _read(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\Desktop\\hiclass_repo\\hiclass\\load.py:79\u001b[0m, in \u001b[0;36mDataset.__init__.<locals>._read\u001b[1;34m(which)\u001b[0m\n\u001b[0;32m     75\u001b[0m             d \u001b[38;5;241m=\u001b[39m handle_nan\u001b[38;5;241m.\u001b[39mimpute_mean(d)\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m d\n\u001b[1;32m---> 79\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43m_read_arff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m remove(file)\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32m~\\Desktop\\hiclass_repo\\hiclass\\load.py:65\u001b[0m, in \u001b[0;36mDataset.__init__.<locals>._read.<locals>._read_arff\u001b[1;34m(f)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# might be a problem when \"mean\" imputing one-hot encoded category with > 2 levels\u001b[39;00m\n\u001b[0;32m     63\u001b[0m d \u001b[38;5;241m=\u001b[39m OneHotEncoder()\u001b[38;5;241m.\u001b[39mfit_transform(d)\n\u001b[1;32m---> 65\u001b[0m d[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclass\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: [label \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m@\u001b[39m\u001b[38;5;124m\"\u001b[39m)])\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# touches also test data, but since HiClass classifiers cannot handle NaN values, this has to be\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# done anyway\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nan_strategy \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mremove\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m which \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\Desktop\\hiclass_repo\\hiclass\\venv\\Lib\\site-packages\\scipy\\sparse\\_index.py:46\u001b[0m, in \u001b[0;36mIndexMixin.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m---> 46\u001b[0m     row, col \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;66;03m# Dispatch to specialized methods.\u001b[39;00m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(row, INT_TYPES):\n",
      "File \u001b[1;32m~\\Desktop\\hiclass_repo\\hiclass\\venv\\Lib\\site-packages\\scipy\\sparse\\_index.py:158\u001b[0m, in \u001b[0;36mIndexMixin._validate_indices\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    156\u001b[0m         row \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m M\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(row, \u001b[38;5;28mslice\u001b[39m):\n\u001b[1;32m--> 158\u001b[0m     row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_asindices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mM\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m isintlike(col):\n\u001b[0;32m    161\u001b[0m     col \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(col)\n",
      "File \u001b[1;32m~\\Desktop\\hiclass_repo\\hiclass\\venv\\Lib\\site-packages\\scipy\\sparse\\_index.py:182\u001b[0m, in \u001b[0;36mIndexMixin._asindices\u001b[1;34m(self, idx, length)\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minvalid index\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m--> 182\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIndex dimension must be 1 or 2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[1;31mIndexError\u001b[0m: Index dimension must be 1 or 2"
     ]
    }
   ],
   "source": [
    "dataset_name = 'cellcycle'  # Replace with your dataset name\n",
    "dataset = Dataset(dataset_name, nan_strategy='mean')\n",
    "\n",
    "# Load train and valid data\n",
    "train_df = dataset.train\n",
    "valid_df = dataset.valid\n",
    "test_df = dataset.test\n",
    "\n",
    "# Generate and store noisy datasets in a list\n",
    "noisy_datasets = generate_noisy_datasets(train_df, valid_df, num_variants=10)\n",
    "\n",
    "# Display the first few rows of the first noisy train and valid dataset as an example\n",
    "noisy_train_1, noisy_valid_1 = noisy_datasets[1]\n",
    "noisy_train_1.head(), noisy_valid_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05da5e9c-1172-413d-8f91-90e574bdad40",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b085054a-4ae7-41ef-b7c8-054cc7f563fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "def scale_data(df, method='standard'):\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if method == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    elif method == 'minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "    df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "    return df\n",
    "\n",
    "# Outliers handling options\n",
    "\n",
    "def remove_outliers_zscore(df, threshold=3.0):\n",
    "    z_scores = np.abs(stats.zscore(df.select_dtypes(include=[np.number])))\n",
    "    filtered_entries = (z_scores < threshold).all(axis=1)\n",
    "    return df[filtered_entries]\n",
    "\n",
    "def remove_outliers_iqr(df):\n",
    "    Q1 = df.quantile(0.25)\n",
    "    Q3 = df.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    filter = (df >= (Q1 - 1.5 * IQR)) & (df <= (Q3 + 1.5 * IQR))\n",
    "    return df[filter.all(axis=1)]\n",
    "\n",
    "# Imputing Outliers\n",
    "def clip_outliers(df, lower_percentile=0.01, upper_percentile=0.99):\n",
    "    lower_bound = df.quantile(lower_percentile)\n",
    "    upper_bound = df.quantile(upper_percentile)\n",
    "    return df.clip(lower=lower_bound, upper=upper_bound, axis=1)\n",
    "\n",
    "def replace_outliers_with_mean(df, threshold=3.0):\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        mean = df[col].mean()\n",
    "        std = df[col].std()\n",
    "        outliers = np.abs((df[col] - mean) / std) > threshold\n",
    "        df.loc[outliers, col] = mean\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1e77d5-bd8e-427b-8784-2088ae5c36e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from hiclass.MultiLabelLocalClassifierPerNode import MultiLabelLocalClassifierPerNode\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "class NoisePreprocessor(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, scaling_method='standard', outlier_method='zscore', imputation_method='mode', outlier_threshold=3.0):\n",
    "        self.scaling_method = scaling_method\n",
    "        self.outlier_method = outlier_method\n",
    "        self.imputation_method = imputation_method\n",
    "        self.outlier_threshold = outlier_threshold\n",
    "        self.scaler_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if self.outlier_method in ['zscore', 'iqr', 'clip', 'mean']:\n",
    "            X_clean, _ = self._remove_outliers(X, y)\n",
    "        else:\n",
    "            X_clean = X\n",
    "\n",
    "        self.scaler_ = StandardScaler() if self.scaling_method == 'standard' else MinMaxScaler()\n",
    "        self.scaler_.fit(X_clean.select_dtypes(include=[np.number]))\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "\n",
    "        if self.outlier_method in ['zscore', 'iqr', 'clip', 'mean']:\n",
    "            X, y = self._remove_outliers(X, y)\n",
    "        \n",
    "        X = self._scale_data(X)\n",
    "\n",
    "        if self.imputation_method in ['mode', 'mean']:\n",
    "            X = self._impute_nominal_values(X)\n",
    "        \n",
    "        if y is not None:\n",
    "            return X, y\n",
    "        return X\n",
    "\n",
    "    def _remove_outliers(self, df, y=None):\n",
    "        if self.outlier_method == 'zscore':\n",
    "            return self._remove_outliers_zscore(df, y)\n",
    "        elif self.outlier_method == 'iqr':\n",
    "            return self._remove_outliers_iqr(df, y)\n",
    "        elif self.outlier_method == 'clip':\n",
    "            return self._clip_outliers(df, y)\n",
    "        elif self.outlier_method == 'mean':\n",
    "            return self._replace_outliers_with_mean(df, y)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown outlier method: {self.outlier_method}\")\n",
    "\n",
    "    def _remove_outliers_zscore(self, df, y=None):\n",
    "        numeric_cols = df.select_dtypes(include=[np.number])\n",
    "        z_scores = np.abs(stats.zscore(numeric_cols))\n",
    "        filtered_entries = (z_scores < self.outlier_threshold).all(axis=1)\n",
    "        if y is not None:\n",
    "            return df.loc[filtered_entries], y.loc[filtered_entries]\n",
    "        return df.loc[filtered_entries], y\n",
    "\n",
    "    def _remove_outliers_iqr(self, df, y=None):\n",
    "        Q1 = df.quantile(0.25)\n",
    "        Q3 = df.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        filter = (df >= (Q1 - 1.5 * IQR)) & (df <= (Q3 + 1.5 * IQR))\n",
    "        filtered_entries = filter.all(axis=1)\n",
    "        if y is not None:\n",
    "            return df.loc[filtered_entries], y.loc[filtered_entries]\n",
    "        return df.loc[filtered_entries], y\n",
    "\n",
    "    def _clip_outliers(self, df, y=None, lower_percentile=0.01, upper_percentile=0.99):\n",
    "        lower_bound = df.quantile(lower_percentile)\n",
    "        upper_bound = df.quantile(upper_percentile)\n",
    "        df_clipped = df.clip(lower=lower_bound, upper=upper_bound, axis=1)\n",
    "        return df_clipped, y\n",
    "\n",
    "    def _replace_outliers_with_mean(self, df, y=None):\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        for col in numeric_cols:\n",
    "            mean = df[col].mean()\n",
    "            std = df[col].std()\n",
    "            outliers = np.abs((df[col] - mean) / std) > self.outlier_threshold\n",
    "            df.loc[outliers, col] = mean\n",
    "        return df, y\n",
    "\n",
    "    def _scale_data(self, df):\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        df.loc[:, numeric_cols] = self.scaler_.transform(df[numeric_cols])\n",
    "        return df\n",
    "\n",
    "    def _impute_nominal_values(self, df):\n",
    "        nominal_cols = df.select_dtypes(include=['category', 'object']).columns\n",
    "        for col in nominal_cols:\n",
    "            if self.imputation_method == 'mode':\n",
    "                mode_value = df[col].mode()[0]\n",
    "                df[col].fillna(mode_value, inplace=True)\n",
    "            elif self.imputation_method == 'mean' and df[col].dtype == np.number:\n",
    "                mean_value = df[col].mean()\n",
    "                df[col].fillna(mean_value, inplace=True)\n",
    "        return df\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', NoisePreprocessor(\n",
    "        scaling_method='standard', \n",
    "        outlier_method='mean', \n",
    "        imputation_method='mode', \n",
    "        outlier_threshold=3.0\n",
    "    )),\n",
    "    ('classifier', MultiLabelLocalClassifierPerNode(\n",
    "        local_classifier=LogisticRegression(\n",
    "            penalty='l2',\n",
    "            C=0.01,\n",
    "            solver='lbfgs',\n",
    "            max_iter=10000\n",
    "        )\n",
    "    ))\n",
    "])\n",
    "\n",
    "pipeline.fit(dataset.x_train(), dataset.y_train())\n",
    "\n",
    "y_pred_valid = pipeline.predict(dataset.x_valid())\n",
    "valid_accuracy = f1_score(dataset.y_valid(), y_pred_valid)\n",
    "\n",
    "y_pred_test = pipeline.predict(dataset_x.test())\n",
    "test_accuracy = f1_score(dataset_y.test(), y_pred_test)\n",
    "\n",
    "results = {\n",
    "    'valid_accuracy': valid_accuracy,\n",
    "    'test_accuracy': test_accuracy\n",
    "}\n",
    "\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
