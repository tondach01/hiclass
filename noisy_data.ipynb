{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69ec9bec-43c3-4b6d-a1a6-b68ce50ac711",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from load import Dataset\n",
    "\n",
    "def add_nominal_noise(df, percentage):\n",
    "    noisy_df = df.copy()\n",
    "    nominal_columns = df.select_dtypes(include=['category', 'object']).columns\n",
    "    for col in nominal_columns:\n",
    "        if col == 'class':  # Skip the 'class' column\n",
    "            continue\n",
    "        unique_values = df[col].apply(lambda x: tuple(x) if isinstance(x, list) else x).unique()\n",
    "        indices = noisy_df.sample(frac=percentage / 100).index\n",
    "        noisy_df.loc[indices, col] = np.random.choice(unique_values, size=len(indices))\n",
    "    return noisy_df\n",
    "\n",
    "def add_numeric_noise(df, percentage, add_noise=True):\n",
    "    noisy_df = df.copy()\n",
    "    numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_columns:\n",
    "        noise = (df[col].max() - df[col].min()) * (percentage / 100)\n",
    "        if add_noise:\n",
    "            noisy_df[col] = df[col] + noise\n",
    "        else:\n",
    "            noisy_df[col] = df[col] - noise\n",
    "    return noisy_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed0ea2e0-ea7e-4e33-8d02-2d70dbe6fd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_noisy_datasets(train_df, valid_df, num_variants=10):\n",
    "    noisy_datasets = []\n",
    "    for i in range(1, 11):\n",
    "        # Add noise\n",
    "        train_noisy_add = add_numeric_noise(train_df, i, add_noise=True)\n",
    "        valid_noisy_add = add_numeric_noise(valid_df, i, add_noise=True)\n",
    "        noisy_datasets.append((train_noisy_add, valid_noisy_add))\n",
    "        \n",
    "        # Subtract noise\n",
    "        train_noisy_subtract = add_numeric_noise(train_df, i, add_noise=False)\n",
    "        valid_noisy_subtract = add_numeric_noise(valid_df, i, add_noise=False)\n",
    "        noisy_datasets.append((train_noisy_subtract, valid_noisy_subtract))\n",
    "    return noisy_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62634248-496e-41ce-b5e9-268334234735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   cln3-1    cln3-2  clb2-2  clb2-1  alpha0  alpha7  alpha14  alpha21  \\\n",
       " 0  0.0733 -0.282872 -0.2916 -0.0096  -0.222 -0.2178  -0.2491   0.1301   \n",
       " 1 -1.2967 -0.333100 -0.1716  0.1504  -0.212 -0.7778   0.0609  -0.3599   \n",
       " 2 -0.6767  0.946900  0.1684  0.5704  -0.122 -0.5978  -0.5091  -0.0999   \n",
       " 3  0.1733 -0.853100 -0.2916 -0.6196  -0.102 -0.3378   0.1309  -0.1599   \n",
       " 4 -0.1967 -0.603100 -0.1916 -0.2596  -0.072 -0.0778   0.0809  -0.2699   \n",
       " \n",
       "    alpha28  alpha35  ...  elu150  elu180  elu210  elu240  elu270  elu300  \\\n",
       " 0  -0.4498  -0.4725  ...   0.083 -0.1454  0.3454  0.0432 -0.1176 -0.3589   \n",
       " 1  -0.4298  -0.6125  ...   0.253  0.1246  0.3754  0.2132  0.1924 -0.3089   \n",
       " 2   0.0802  -0.1025  ...  -0.857 -0.2854 -0.1846  0.2732  0.1824 -0.4689   \n",
       " 3  -0.2998   0.0275  ...   0.093  0.2646 -0.1246 -0.4368 -0.6776 -0.1089   \n",
       " 4  -0.1598   0.2175  ...  -0.427 -0.2054  0.0954  0.3032 -0.0576 -0.1989   \n",
       " \n",
       "    elu330  elu360  elu390                                              class  \n",
       " 0  0.0054 -0.5065  0.0131                  [11/02/01, 11/02/02, 11/02/03/01]  \n",
       " 1 -0.1346  0.3135 -0.0069                                         [12/04/02]  \n",
       " 2  0.1754  0.5735  0.6231  [01/04, 14/01, 14/04, 16/01, 16/19/03, 20/09/0...  \n",
       " 3 -0.3846 -0.2165  0.1231                     [10/03/02, 42/10, 43/01/03/09]  \n",
       " 4  0.2954  0.2535 -0.0169                               [10/01/05/01, 32/01]  \n",
       " \n",
       " [5 rows x 78 columns],\n",
       "    cln3-1  cln3-2  clb2-2  clb2-1  alpha0  alpha7  alpha14  alpha21  alpha28  \\\n",
       " 0 -0.1459 -0.8344 -0.1945 -0.3072 -0.1548    0.05   -0.023   0.0211   0.0113   \n",
       " 1 -0.3759 -0.6744 -0.2545 -0.0472 -0.0948    0.08    0.097  -0.2489  -0.4787   \n",
       " 2 -1.2959  0.5856 -0.7145 -0.2272 -0.0248   -0.38   -0.523  -0.3389  -0.1787   \n",
       " 3  0.3641 -0.5844 -0.2945  0.0928 -0.0648   -0.34   -0.103  -0.2589  -0.0887   \n",
       " 4  0.9041  1.2556 -0.3445 -0.3472 -0.6148   -1.17   -0.243   0.0611   0.6013   \n",
       " \n",
       "    alpha35  ...  elu150  elu180  elu210  elu240  elu270  elu300  elu330  \\\n",
       " 0  -0.2942  ... -0.1057 -0.1518 -0.2306 -0.0816 -0.1242 -0.2145  0.0596   \n",
       " 1  -0.2442  ... -0.1957  0.3482 -0.1006 -0.0316 -0.0042  0.3155  0.0296   \n",
       " 2  -0.2742  ...  0.1443  0.2482  0.0094  0.0684  0.4458 -0.1045  0.0896   \n",
       " 3  -0.1042  ... -0.2257  0.2482  0.2694 -0.3616 -0.0742  0.2055  0.0596   \n",
       " 4   0.3758  ...  0.1443  0.4682  0.6194  0.5084  0.5558 -0.0545  0.4696   \n",
       " \n",
       "    elu360  elu390                                class  \n",
       " 0  -0.017  -0.037                    [14/04, 20/09/13]  \n",
       " 1  -0.957   0.033                              [42/16]  \n",
       " 2  -0.437   0.143  [01/01/06/05/01/01, 01/01/09/03/01]  \n",
       " 3  -0.477   0.233       [10/03/05/01, 42/04/05, 42/29]  \n",
       " 4   0.013  -0.047                 [01/05, 14/07/02/01]  \n",
       " \n",
       " [5 rows x 78 columns])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = 'cellcycle'  # Replace with your dataset name\n",
    "dataset = Dataset(dataset_name, nan_strategy='mean')\n",
    "\n",
    "# Load train and valid data\n",
    "train_df = dataset.train\n",
    "valid_df = dataset.valid\n",
    "test_df = dataset.test\n",
    "\n",
    "# Generate and store noisy datasets in a list\n",
    "noisy_datasets = generate_noisy_datasets(train_df, valid_df, num_variants=10)\n",
    "\n",
    "# Display the first few rows of the first noisy train and valid dataset as an example\n",
    "noisy_train_1, noisy_valid_1 = noisy_datasets[1]\n",
    "noisy_train_1.head(), noisy_valid_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05da5e9c-1172-413d-8f91-90e574bdad40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cln3-1</th>\n",
       "      <th>cln3-2</th>\n",
       "      <th>clb2-2</th>\n",
       "      <th>clb2-1</th>\n",
       "      <th>alpha0</th>\n",
       "      <th>alpha7</th>\n",
       "      <th>alpha14</th>\n",
       "      <th>alpha21</th>\n",
       "      <th>alpha28</th>\n",
       "      <th>alpha35</th>\n",
       "      <th>...</th>\n",
       "      <th>elu150</th>\n",
       "      <th>elu180</th>\n",
       "      <th>elu210</th>\n",
       "      <th>elu240</th>\n",
       "      <th>elu270</th>\n",
       "      <th>elu300</th>\n",
       "      <th>elu330</th>\n",
       "      <th>elu360</th>\n",
       "      <th>elu390</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.219772</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>...</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>0.04</td>\n",
       "      <td>[11/02/01, 11/02/02, 11/02/03/01]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.22</td>\n",
       "      <td>-0.270000</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.23</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.71</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.58</td>\n",
       "      <td>...</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.22</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.02</td>\n",
       "      <td>[12/04/02]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.60</td>\n",
       "      <td>1.010000</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.65</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.53</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.83</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.21</td>\n",
       "      <td>-0.43</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.65</td>\n",
       "      <td>[01/04, 14/01, 14/04, 16/01, 16/19/03, 20/09/0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.25</td>\n",
       "      <td>-0.790000</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>-0.54</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>0.06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>-0.65</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>0.15</td>\n",
       "      <td>[10/03/02, 42/10, 43/01/03/09]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.540000</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.33</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[10/01/05/01, 32/01]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cln3-1    cln3-2  clb2-2  clb2-1  alpha0  alpha7  alpha14  alpha21  \\\n",
       "0    0.15 -0.219772   -0.22    0.07   -0.15   -0.15    -0.21     0.17   \n",
       "1   -1.22 -0.270000   -0.10    0.23   -0.14   -0.71     0.10    -0.32   \n",
       "2   -0.60  1.010000    0.24    0.65   -0.05   -0.53    -0.47    -0.06   \n",
       "3    0.25 -0.790000   -0.22   -0.54   -0.03   -0.27     0.17    -0.12   \n",
       "4   -0.12 -0.540000   -0.12   -0.18    0.00   -0.01     0.12    -0.23   \n",
       "\n",
       "   alpha28  alpha35  ...  elu150  elu180  elu210  elu240  elu270  elu300  \\\n",
       "0    -0.42    -0.44  ...    0.11   -0.12    0.37    0.07   -0.09   -0.32   \n",
       "1    -0.40    -0.58  ...    0.28    0.15    0.40    0.24    0.22   -0.27   \n",
       "2     0.11    -0.07  ...   -0.83   -0.26   -0.16    0.30    0.21   -0.43   \n",
       "3    -0.27     0.06  ...    0.12    0.29   -0.10   -0.41   -0.65   -0.07   \n",
       "4    -0.13     0.25  ...   -0.40   -0.18    0.12    0.33   -0.03   -0.16   \n",
       "\n",
       "   elu330  elu360  elu390                                              class  \n",
       "0    0.04   -0.48    0.04                  [11/02/01, 11/02/02, 11/02/03/01]  \n",
       "1   -0.10    0.34    0.02                                         [12/04/02]  \n",
       "2    0.21    0.60    0.65  [01/04, 14/01, 14/04, 16/01, 16/19/03, 20/09/0...  \n",
       "3   -0.35   -0.19    0.15                     [10/03/02, 42/10, 43/01/03/09]  \n",
       "4    0.33    0.28    0.01                               [10/01/05/01, 32/01]  \n",
       "\n",
       "[5 rows x 78 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b085054a-4ae7-41ef-b7c8-054cc7f563fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "def scale_data(df, method='standard'):\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if method == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    elif method == 'minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "    df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "    return df\n",
    "\n",
    "# Outliers handling options\n",
    "\n",
    "def remove_outliers_zscore(df, threshold=3.0):\n",
    "    z_scores = np.abs(stats.zscore(df.select_dtypes(include=[np.number])))\n",
    "    filtered_entries = (z_scores < threshold).all(axis=1)\n",
    "    return df[filtered_entries]\n",
    "\n",
    "def remove_outliers_iqr(df):\n",
    "    Q1 = df.quantile(0.25)\n",
    "    Q3 = df.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    filter = (df >= (Q1 - 1.5 * IQR)) & (df <= (Q3 + 1.5 * IQR))\n",
    "    return df[filter.all(axis=1)]\n",
    "\n",
    "# Imputing Outliers\n",
    "def clip_outliers(df, lower_percentile=0.01, upper_percentile=0.99):\n",
    "    lower_bound = df.quantile(lower_percentile)\n",
    "    upper_bound = df.quantile(upper_percentile)\n",
    "    return df.clip(lower=lower_bound, upper=upper_bound, axis=1)\n",
    "\n",
    "def replace_outliers_with_mean(df, threshold=3.0):\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        mean = df[col].mean()\n",
    "        std = df[col].std()\n",
    "        outliers = np.abs((df[col] - mean) / std) > threshold\n",
    "        df.loc[outliers, col] = mean\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a1e77d5-bd8e-427b-8784-2088ae5c36e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1073, 1628]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 119\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[0;32m    102\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[0;32m    103\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessor\u001b[39m\u001b[38;5;124m'\u001b[39m, NoisePreprocessor(\n\u001b[0;32m    104\u001b[0m         scaling_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstandard\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    116\u001b[0m     ))\n\u001b[0;32m    117\u001b[0m ])\n\u001b[1;32m--> 119\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m y_pred_valid \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mpredict(dataset\u001b[38;5;241m.\u001b[39mx_valid())\n\u001b[0;32m    122\u001b[0m valid_accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(dataset\u001b[38;5;241m.\u001b[39my_valid(), y_pred_valid)\n",
      "File \u001b[1;32m~\\Desktop\\hiclass_repo\\hiclass\\venv\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\hiclass_repo\\hiclass\\venv\\Lib\\site-packages\\sklearn\\pipeline.py:475\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    474\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m routed_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m--> 475\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\Desktop\\hiclass_repo\\hiclass\\venv\\Lib\\site-packages\\hiclass\\MultiLabelLocalClassifierPerNode.py:133\u001b[0m, in \u001b[0;36mMultiLabelLocalClassifierPerNode.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03mFit a local classifier per node.\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    Fitted estimator.\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;66;03m# Execute common methods necessary before fitting\u001b[39;00m\n\u001b[1;32m--> 133\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pre_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# Initialize policy\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_binary_policy()\n",
      "File \u001b[1;32m~\\Desktop\\hiclass_repo\\hiclass\\venv\\Lib\\site-packages\\hiclass\\MultiLabelHierarchicalClassifier.py:160\u001b[0m, in \u001b[0;36mMultiLabelHierarchicalClassifier._pre_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_pre_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight):\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;66;03m# Check that X and y have correct shape\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# and convert them to np.ndarray if need be\u001b[39;00m\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert:\n\u001b[1;32m--> 160\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m            \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    164\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(X)\n",
      "File \u001b[1;32m~\\Desktop\\hiclass_repo\\hiclass\\venv\\Lib\\site-packages\\sklearn\\base.py:650\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    648\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 650\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\Desktop\\hiclass_repo\\hiclass\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1281\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1263\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1264\u001b[0m     X,\n\u001b[0;32m   1265\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1276\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1277\u001b[0m )\n\u001b[0;32m   1279\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m-> 1281\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1283\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[1;32m~\\Desktop\\hiclass_repo\\hiclass\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    455\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    459\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    460\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1073, 1628]"
     ]
    }
   ],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from hiclass.MultiLabelLocalClassifierPerNode import MultiLabelLocalClassifierPerNode\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "class NoisePreprocessor(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, scaling_method='standard', outlier_method='zscore', imputation_method='mode', outlier_threshold=3.0):\n",
    "        self.scaling_method = scaling_method\n",
    "        self.outlier_method = outlier_method\n",
    "        self.imputation_method = imputation_method\n",
    "        self.outlier_threshold = outlier_threshold\n",
    "        self.scaler_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if self.outlier_method in ['zscore', 'iqr', 'clip', 'mean']:\n",
    "            X_clean, _ = self._remove_outliers(X, y)\n",
    "        else:\n",
    "            X_clean = X\n",
    "\n",
    "        self.scaler_ = StandardScaler() if self.scaling_method == 'standard' else MinMaxScaler()\n",
    "        self.scaler_.fit(X_clean.select_dtypes(include=[np.number]))\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "\n",
    "        if self.outlier_method in ['zscore', 'iqr', 'clip', 'mean']:\n",
    "            X, y = self._remove_outliers(X, y)\n",
    "        \n",
    "        X = self._scale_data(X)\n",
    "\n",
    "        if self.imputation_method in ['mode', 'mean']:\n",
    "            X = self._impute_nominal_values(X)\n",
    "        \n",
    "        if y is not None:\n",
    "            return X, y\n",
    "        return X\n",
    "\n",
    "    def _remove_outliers(self, df, y=None):\n",
    "        if self.outlier_method == 'zscore':\n",
    "            return self._remove_outliers_zscore(df, y)\n",
    "        elif self.outlier_method == 'iqr':\n",
    "            return self._remove_outliers_iqr(df, y)\n",
    "        elif self.outlier_method == 'clip':\n",
    "            return self._clip_outliers(df, y)\n",
    "        elif self.outlier_method == 'mean':\n",
    "            return self._replace_outliers_with_mean(df, y)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown outlier method: {self.outlier_method}\")\n",
    "\n",
    "    def _remove_outliers_zscore(self, df, y=None):\n",
    "        numeric_cols = df.select_dtypes(include=[np.number])\n",
    "        z_scores = np.abs(stats.zscore(numeric_cols))\n",
    "        filtered_entries = (z_scores < self.outlier_threshold).all(axis=1)\n",
    "        if y is not None:\n",
    "            return df.loc[filtered_entries], y.loc[filtered_entries]\n",
    "        return df.loc[filtered_entries], y\n",
    "\n",
    "    def _remove_outliers_iqr(self, df, y=None):\n",
    "        Q1 = df.quantile(0.25)\n",
    "        Q3 = df.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        filter = (df >= (Q1 - 1.5 * IQR)) & (df <= (Q3 + 1.5 * IQR))\n",
    "        filtered_entries = filter.all(axis=1)\n",
    "        if y is not None:\n",
    "            return df.loc[filtered_entries], y.loc[filtered_entries]\n",
    "        return df.loc[filtered_entries], y\n",
    "\n",
    "    def _clip_outliers(self, df, y=None, lower_percentile=0.01, upper_percentile=0.99):\n",
    "        lower_bound = df.quantile(lower_percentile)\n",
    "        upper_bound = df.quantile(upper_percentile)\n",
    "        df_clipped = df.clip(lower=lower_bound, upper=upper_bound, axis=1)\n",
    "        return df_clipped, y\n",
    "\n",
    "    def _replace_outliers_with_mean(self, df, y=None):\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        for col in numeric_cols:\n",
    "            mean = df[col].mean()\n",
    "            std = df[col].std()\n",
    "            outliers = np.abs((df[col] - mean) / std) > self.outlier_threshold\n",
    "            df.loc[outliers, col] = mean\n",
    "        return df, y\n",
    "\n",
    "    def _scale_data(self, df):\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        df.loc[:, numeric_cols] = self.scaler_.transform(df[numeric_cols])\n",
    "        return df\n",
    "\n",
    "    def _impute_nominal_values(self, df):\n",
    "        nominal_cols = df.select_dtypes(include=['category', 'object']).columns\n",
    "        for col in nominal_cols:\n",
    "            if self.imputation_method == 'mode':\n",
    "                mode_value = df[col].mode()[0]\n",
    "                df[col].fillna(mode_value, inplace=True)\n",
    "            elif self.imputation_method == 'mean' and df[col].dtype == np.number:\n",
    "                mean_value = df[col].mean()\n",
    "                df[col].fillna(mean_value, inplace=True)\n",
    "        return df\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', NoisePreprocessor(\n",
    "        scaling_method='standard', \n",
    "        outlier_method='zscore', \n",
    "        imputation_method='mode', \n",
    "        outlier_threshold=3.0\n",
    "    )),\n",
    "    ('classifier', MultiLabelLocalClassifierPerNode(\n",
    "        local_classifier=LogisticRegression(\n",
    "            penalty='l2',\n",
    "            C=0.01,\n",
    "            solver='lbfgs',\n",
    "            max_iter=10000\n",
    "        )\n",
    "    ))\n",
    "])\n",
    "\n",
    "pipeline.fit(dataset.x_train(), dataset.y_train())\n",
    "\n",
    "y_pred_valid = pipeline.predict(dataset.x_valid())\n",
    "valid_accuracy = accuracy_score(dataset.y_valid(), y_pred_valid)\n",
    "\n",
    "y_pred_test = pipeline.predict(dataset_x.test())\n",
    "test_accuracy = accuracy_score(dataset_y.test(), y_pred_test)\n",
    "\n",
    "results = {\n",
    "    'valid_accuracy': valid_accuracy,\n",
    "    'test_accuracy': test_accuracy,\n",
    "    'valid_classification_report': classification_report(valid_df['class'], y_pred_valid),\n",
    "    'test_classification_report': classification_report(test_df['class'], y_pred_test),\n",
    "}\n",
    "\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
