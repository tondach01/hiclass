{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install git+https://github.com/Tomko10/hiclass.git scikit-learn numpy pandas matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### handle_nan.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, Union\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "\n",
    "\n",
    "def impute_mean(x: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Impute NaN values by simple column mean\n",
    "\n",
    "    :param x: data to be imputed\n",
    "    :return: new DataFrame with values imputed\n",
    "    \"\"\"\n",
    "    numeric = x.select_dtypes(include=[\"number\"])\n",
    "    rest = x.select_dtypes(exclude=[\"number\"])\n",
    "    imp = SimpleImputer(strategy=\"mean\").fit(numeric)\n",
    "    return pd.concat([pd.DataFrame(imp.transform(numeric), columns=imp.feature_names_in_), rest], axis=1)\n",
    "\n",
    "\n",
    "def impute_knn(x: pd.DataFrame, k=5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Impute NaN values by KNN algorithm\n",
    "\n",
    "    :param x: data to be imputed\n",
    "    :param k: number of nearest neighbors\n",
    "    :return: new DataFrame with values imputed\n",
    "    \"\"\"\n",
    "    numeric = x.select_dtypes(include=[\"number\"])\n",
    "    rest = x.select_dtypes(exclude=[\"number\"])\n",
    "    imp = KNNImputer(n_neighbors=k).fit(numeric)\n",
    "    return pd.concat([pd.DataFrame(imp.transform(numeric), columns=imp.feature_names_in_), rest], axis=1)\n",
    "\n",
    "\n",
    "# disclaimer: may remove all the rows\n",
    "def remove_nan(x: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove rows with NaN values from the data\n",
    "\n",
    "    :param x: data, possibly containing NaN values\n",
    "    :return: features and labels without rows containing NaN\n",
    "    \"\"\"\n",
    "    nan_rows = x[x.isnull().T.any()].index\n",
    "    return x.drop(labels=nan_rows)\n",
    "\n",
    "\n",
    "ImputerStrategy = Union[Literal[\"drop\"],\n",
    "                        Literal[\"knn\"],\n",
    "                        Literal[\"mean\"],\n",
    "                        Literal[\"median\"],\n",
    "                        Literal[\"most_frequent\"],\n",
    "                        Literal[\"constant\"]]\n",
    "\n",
    "\n",
    "class NumericImputer(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, strategy: ImputerStrategy = 'mean', **kwargs):\n",
    "        self.strategy = strategy\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.numeric_columns_ = X.select_dtypes(include=[\"number\"]).columns\n",
    "        self.rest_columns_ = X.select_dtypes(exclude=[\"number\"]).columns\n",
    "\n",
    "        if self.strategy == \"knn\":\n",
    "            if \"n_neighbors\" not in self.kwargs:\n",
    "                self.kwargs[\"n_neighbors\"] = 5\n",
    "            self.imputer_ = KNNImputer(**self.kwargs)\n",
    "        elif self.strategy == \"drop\":\n",
    "            self.imputer_ = None\n",
    "        else:\n",
    "            self.imputer_ = SimpleImputer(strategy=self.strategy,\n",
    "                                          **self.kwargs)\n",
    "\n",
    "        self.imputer_.fit(X[self.numeric_columns_])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.strategy == \"drop\":\n",
    "            return X.dropna()\n",
    "\n",
    "        numeric = X[self.numeric_columns_]\n",
    "        rest = X[self.rest_columns_]\n",
    "        numeric_imputed = pd.DataFrame(self.imputer_.transform(numeric),\n",
    "                                       columns=self.numeric_columns_)\n",
    "        return pd.concat([numeric_imputed, rest], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from os import sep, remove\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    \"\"\"\n",
    "    Represents ARFF dataset for hierarchical classification.\n",
    "\n",
    "    Requires directory structure as follows:\n",
    "\n",
    "        load.py\n",
    "        datasets_FUN -> XXX_FUN -> XXX_FUN.{train,test,valid}.arff.zip\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset_name: str, nan_strategy: str = \"mean\", args=None):\n",
    "        \"\"\"\n",
    "        Create Dataset object, consisting of training/testing/validation data\n",
    "\n",
    "        :param dataset_name: name of the dataset (without _FUN suffix) - one of {cellcycle, church, derisi, eisen,\n",
    "        expr, gasch1, gasch2, hom, pheno, seq, spo, struc}\n",
    "        :param nan_strategy: strategy to be used for NaN values - one of \"mean\", \"knn\", \"remove\". If not\n",
    "        provided or not one of allowed, \"mean\" is used\n",
    "        :param args: possible dictionary of arguments to NaN-handling functions\n",
    "\n",
    "        the hom_FUN dataset is quite large and takes a lot of time to process\n",
    "        struc_FUN takes moderate amount of time (around 5 minutes on my laptop)\n",
    "        \"\"\"\n",
    "        path = sep.join([\"datasets_FUN\", f\"{dataset_name}_FUN\"])\n",
    "\n",
    "        def _read(which: str) -> pd.DataFrame:\n",
    "            file = f\"{dataset_name}_FUN.{which}.arff\"\n",
    "            p = sep.join([path, file+\".zip\"])\n",
    "            zipfile.ZipFile(p).extract(file)\n",
    "\n",
    "            # Apparently, scipy cannot read hierarchical attributes\n",
    "            def _read_arff(f: str) -> pd.DataFrame:\n",
    "                attr_names = []\n",
    "                with open(f) as arff_file:\n",
    "                    reading_attrs = True\n",
    "                    types = {\"class\": \"object\"}\n",
    "                    # gasch1 dataset has two columns of the same name\n",
    "                    used_names, i = set(), 0\n",
    "                    while reading_attrs:\n",
    "                        attr = arff_file.readline().strip().split()\n",
    "                        if (not attr) or (attr[0].upper() != \"@ATTRIBUTE\"):\n",
    "                            if attr and attr[0].upper() == \"@DATA\":\n",
    "                                reading_attrs = False\n",
    "                            continue\n",
    "                        if attr[1] in used_names:\n",
    "                            attr[1] += f\"_{i}\"\n",
    "                            i += 1\n",
    "                        attr_names.append(attr[1])\n",
    "                        used_names.add(attr[1])\n",
    "                        if attr[2].startswith(\"{\"):\n",
    "                            types[attr[1]] = \"category\"\n",
    "\n",
    "                    d = pd.read_csv(arff_file, names=attr_names, na_values=[\"?\"], dtype=types)\n",
    "\n",
    "                    # might be a problem when \"mean\" imputing one-hot encoded category with > 2 levels\n",
    "                    categories = [column for column, t in types.items() if t == \"category\"]\n",
    "                    if categories:\n",
    "                        enc = OneHotEncoder(sparse_output=False)\n",
    "                        encoded_columns = enc.fit_transform(d[categories])\n",
    "                        encoded_df = pd.DataFrame(encoded_columns,\n",
    "                                                  columns=enc.get_feature_names_out(categories))\n",
    "                        d = pd.concat([d.drop(columns=categories), encoded_df], axis=1)\n",
    "\n",
    "                    d[\"class\"] = d[\"class\"].map(lambda x: [label for label in x.split(\"@\")])\n",
    "\n",
    "                    # touches also test data, but since HiClass classifiers cannot handle NaN values, this has to be\n",
    "                    # done anyway\n",
    "                    if nan_strategy == \"remove\" and which != \"test\":\n",
    "                        d = remove_nan(d)\n",
    "                    elif nan_strategy == \"knn\":\n",
    "                        k = 5\n",
    "                        if args is not None:\n",
    "                            k = args.get(\"k\", 5)\n",
    "                        d = impute_knn(d, k)\n",
    "                    elif nan_strategy == \"mean\":\n",
    "                        d = impute_mean(d)\n",
    "\n",
    "                return d\n",
    "\n",
    "            data = _read_arff(file)\n",
    "            remove(file)\n",
    "            return data\n",
    "\n",
    "        self.train = _read(\"train\")\n",
    "        self.test = _read(\"test\")\n",
    "        self.valid = _read(\"valid\")\n",
    "\n",
    "    def _x(self, data: pd.DataFrame, expand: bool = False) -> pd.DataFrame:\n",
    "        df = data.copy()\n",
    "        if expand:\n",
    "            df = self.expand_multi_class(df)\n",
    "        df.pop(\"class\")\n",
    "        return df\n",
    "\n",
    "    def _y(self, data: pd.DataFrame, expand: bool = False) -> pd.Series:\n",
    "        df = data.copy()\n",
    "        if expand:\n",
    "            df = self.expand_multi_class(df)\n",
    "            return df[\"class\"].apply(lambda x: x.split(\"/\"))\n",
    "        return df[\"class\"].apply(lambda x: list(map((lambda y: y.split(\"/\")), x)))\n",
    "\n",
    "    def x_train(self, expand: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get features (not classes) of examples in training part of dataset\n",
    "\n",
    "        :param expand: whether to expand multi-label rows to multiple records\n",
    "        :return: copy of dataframe with classes removed\n",
    "        \"\"\"\n",
    "        return self._x(self.train, expand)\n",
    "\n",
    "    def y_train(self, expand: bool = False) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Get classes (not features) of examples in training part of dataset\n",
    "\n",
    "        :param expand: whether to expand multi-label rows to multiple records\n",
    "        :return: copy of dataframe with features removed\n",
    "        \"\"\"\n",
    "        return self._y(self.train, expand)\n",
    "\n",
    "    def x_test(self, expand: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get features (not classes) of examples in test part of dataset\n",
    "\n",
    "        :param expand: whether to expand multi-label rows to multiple records\n",
    "        :return: copy of dataframe with classes removed\n",
    "        \"\"\"\n",
    "        return self._x(self.test, expand)\n",
    "\n",
    "    def y_test(self, expand: bool = False) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Get classes (not features) of examples in test part of dataset\n",
    "\n",
    "        :param expand: whether to expand multi-label rows to multiple records\n",
    "        :return: copy of dataframe with features removed\n",
    "        \"\"\"\n",
    "        return self._y(self.test, expand)\n",
    "\n",
    "    def x_valid(self, expand: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get features (not classes) of examples in validation part of dataset\n",
    "\n",
    "        :param expand: whether to expand multi-label rows to multiple records\n",
    "        :return: copy of dataframe with classes removed\n",
    "        \"\"\"\n",
    "        return self._x(self.valid, expand)\n",
    "\n",
    "    def y_valid(self, expand: bool = False) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Get classes (not features) of examples in validation part of dataset\n",
    "\n",
    "        :param expand: whether to expand multi-label rows to multiple records\n",
    "        :return: copy of dataframe with features removed\n",
    "        \"\"\"\n",
    "        return self._y(self.valid, expand)\n",
    "\n",
    "    @staticmethod\n",
    "    def expand_multi_class(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Expand the dataset so that each row has just one class label\n",
    "\n",
    "        :param df: dataset to be expanded\n",
    "        :return: the same dataset, but multi-label rows are duplicated for each label\n",
    "        \"\"\"\n",
    "        return df.explode(\"class\", ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature_selection.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.feature_selection import (\n",
    "    SelectorMixin,\n",
    "    mutual_info_classif,\n",
    "    SelectKBest,\n",
    ")\n",
    "from math import sqrt, floor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from hiclass.MultiLabelLocalClassifierPerNode import (\n",
    "    MultiLabelLocalClassifierPerNode,\n",
    ")\n",
    "from hiclass.metrics import f1\n",
    "from random import seed\n",
    "\n",
    "\n",
    "def fill_reshape(y) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Transform the multi-label part of the dataset to regular shape, so F1 metric can be used\n",
    "\n",
    "    :param y: labels (not expanded)\n",
    "    :return: array of x=hierarchy, y=labels per example, z=examples\n",
    "    \"\"\"\n",
    "    if isinstance(y, pd.Series):\n",
    "        max_len = y.apply(len).agg(\"max\")\n",
    "        depth = y.apply(lambda x: max(map(len, x))).agg(\"max\")\n",
    "    else:\n",
    "        max_len = max(map(len, y))\n",
    "        depth = max(map(lambda x: max(map(len, x)), y))\n",
    "\n",
    "    return np.array([\n",
    "        [\n",
    "            list(label) + [\"\"] * (depth - len(label))\n",
    "            for label in row\n",
    "        ] + [\n",
    "            [\"\"] * depth\n",
    "        ] * (max_len - len(row))\n",
    "        for row in y\n",
    "    ])\n",
    "\n",
    "\n",
    "class ModSelectKBest(SelectorMixin, BaseEstimator):\n",
    "    \"\"\"\n",
    "    Perform \"flat\" selection of k best parameters based on mutual information. The hierarchy is ignored and labels\n",
    "    worked with as strings\n",
    "\n",
    "    Sample usage:\n",
    "        import load\n",
    "        import feature_selection\n",
    "        from sklearn.tree import DecisionTreeClassifier\n",
    "        from hiclass.MultiLabelLocalClassifierPerNode import MultiLabelLocalClassifierPerNode\n",
    "        from hiclass.metrics import f1\n",
    "\n",
    "        dataset = load.Dataset(\"cellcycle\", nan_strategy=\"mean\")\n",
    "        x_train, y_train = dataset.x_train(), dataset.y_train()\n",
    "        x_test, y_test = dataset.x_test(), dataset.y_test()\n",
    "\n",
    "        tree = DecisionTreeClassifier()\n",
    "        classifier = MultiLabelLocalClassifierPerNode(local_classifier=tree)\n",
    "\n",
    "        selector = ModSelectKBest().fit(x_train, y_train)\n",
    "        x_train = selector.transform(x_train)\n",
    "\n",
    "        classifier.fit(x_train, y_train)\n",
    "\n",
    "        y_pred = classifier.predict(selector.transform(x_test))\n",
    "        print(f1(fill_reshape(y_test), y_pred))\n",
    "        \"\"\"\n",
    "    def __init__(self, *, k=10, sqrt_features=False):\n",
    "        \"\"\"\n",
    "        Create ModSelectKBest object\n",
    "\n",
    "        :param k: number of features to be chosen\n",
    "        :param sqrt_features: take square root of number of features as k\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.sqrt_features = sqrt_features\n",
    "\n",
    "    def set_params(self, k=10, sqrt_features=False) -> 'ModSelectKBest':\n",
    "        self.k = k\n",
    "        self.sqrt_features = sqrt_features\n",
    "        return self\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        self.n_features_in_ = x.shape[1]\n",
    "\n",
    "        y_pd = pd.DataFrame(y, columns=[\"class\"])\n",
    "        x_exp = self._expand_multi_class(pd.concat([x, y_pd], axis=1))\n",
    "        y_exp = x_exp[\"class\"].copy()\n",
    "        x_exp.drop(columns=\"class\", inplace=True)\n",
    "\n",
    "        y_exp = y_exp.map(lambda label: \"/\".join(label))\n",
    "        if self.sqrt_features:\n",
    "            self.k = floor(sqrt(x.shape[1]))\n",
    "        self.selector_ = SelectKBest(mutual_info_classif, k=self.k).fit(x_exp, y_exp)\n",
    "        self.feature_names_in_ = x.columns\n",
    "        return self\n",
    "\n",
    "    def _get_support_mask(self):\n",
    "        check_is_fitted(self)\n",
    "\n",
    "        return self.selector_.get_support(False)\n",
    "\n",
    "    def _more_tags(self):\n",
    "        return {\"requires_y\": True}\n",
    "\n",
    "    @staticmethod\n",
    "    def _expand_multi_class(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Expand the dataset so that each row has just one class label\n",
    "\n",
    "        :param df: dataset to be expanded\n",
    "        :return: the same dataset, but multi-label rows are duplicated for each label\n",
    "        \"\"\"\n",
    "        return df.explode(\"class\", ignore_index=True)\n",
    "\n",
    "\n",
    "class IterativeSelect(SelectorMixin, BaseEstimator):\n",
    "    \"\"\"\n",
    "    Perform iterative selection of k best parameters based on fit to hiclass.MultiLabelLocalClassifierPerNode +\n",
    "    sklearn.tree.DecisionTreeClassifier measured as F1 score.\n",
    "\n",
    "    Selection of feature subset for each epoch is (pseudo)random, thus results may vary if seed is not specified.\n",
    "\n",
    "    Sample usage:\n",
    "        import load\n",
    "        import feature_selection\n",
    "        from sklearn.tree import DecisionTreeClassifier\n",
    "        from hiclass.MultiLabelLocalClassifierPerNode import MultiLabelLocalClassifierPerNode\n",
    "        from hiclass.metrics import f1\n",
    "\n",
    "        dataset = load.Dataset(\"cellcycle\", nan_strategy=\"mean\")\n",
    "\n",
    "        x_train, y_train = dataset.x_train(), dataset.y_train()\n",
    "        x_valid, y_valid = dataset.x_valid(), dataset.y_valid()\n",
    "        x_test, y_test = dataset.x_test(), dataset.y_test()\n",
    "\n",
    "        tree = DecisionTreeClassifier()\n",
    "        classifier = MultiLabelLocalClassifierPerNode(local_classifier=tree)\n",
    "\n",
    "        selector = IterativeSelect(x_valid=x_valid, y_valid=y_valid, r_seed=42).fit(x_train, y_train)\n",
    "        x_train = selector.transform(x_train)\n",
    "\n",
    "        classifier.fit(x_train, y_train)\n",
    "\n",
    "        y_pred = classifier.predict(selector.transform(x_test))\n",
    "        print(f1(fill_reshape(y_test), y_pred))\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 *,\n",
    "                 x_valid: pd.DataFrame,\n",
    "                 y_valid: pd.DataFrame,\n",
    "                 k=10,\n",
    "                 sqrt_features=False,\n",
    "                 epochs=10,\n",
    "                 r_seed=None,\n",
    "                 verbose=False):\n",
    "        \"\"\"\n",
    "        Create IterativeSelect object\n",
    "\n",
    "        :param x_valid: validation data to compare selections - features\n",
    "        :param y_valid: validation data to compare selections - labels\n",
    "        :param k: number of features to choose\n",
    "        :param sqrt_features: choose square root of number of features instead of k\n",
    "        :param epochs: number of iterations\n",
    "        :param r_seed: seed to subset generator\n",
    "        :param verbose: print logs to output\n",
    "        \"\"\"\n",
    "        self.x_valid = x_valid\n",
    "        self.y_valid = y_valid\n",
    "        self.k = k\n",
    "        self.sqrt_features = sqrt_features\n",
    "        self.epochs = epochs\n",
    "        self.r_seed = r_seed\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def set_params(self,\n",
    "                   *,\n",
    "                   x_valid: pd.DataFrame,\n",
    "                   y_valid: pd.DataFrame,\n",
    "                   k=10,\n",
    "                   sqrt_features=False,\n",
    "                   epochs=10,\n",
    "                   r_seed=None) -> 'IterativeSelect':\n",
    "        self.x_valid = x_valid\n",
    "        self.y_valid = y_valid\n",
    "        self.k = k\n",
    "        self.sqrt_features = sqrt_features\n",
    "        self.epochs = epochs\n",
    "        self.r_seed = r_seed\n",
    "        return self\n",
    "\n",
    "    def _get_support_mask(self):\n",
    "        check_is_fitted(self)\n",
    "\n",
    "        mask = np.zeros(len(self.x_valid.columns), dtype=bool)\n",
    "        mask[self.sample_best_] = True\n",
    "        return mask\n",
    "\n",
    "    def fit(self, X, y) -> 'IterativeSelect':\n",
    "        self.n_features_in_ = X.shape[1]\n",
    "        self.feature_names_in_ = X.columns\n",
    "\n",
    "        if self.sqrt_features:\n",
    "            self.k = floor(sqrt(X.shape[1]))\n",
    "\n",
    "        if self.r_seed is not None:\n",
    "            seed(self.r_seed)\n",
    "\n",
    "        y_valid_reshaped = fill_reshape(self.y_valid)\n",
    "\n",
    "        f1_best = 0\n",
    "        self.sample_best_ = []\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "            s = np.zeros(self.n_features_in_, dtype=bool)\n",
    "            s[np.random.choice(self.n_features_in_,\n",
    "                               self.k,\n",
    "                               replace=False)] = True\n",
    "\n",
    "            tree = DecisionTreeClassifier()\n",
    "            classifier = MultiLabelLocalClassifierPerNode(\n",
    "                local_classifier=tree\n",
    "            )\n",
    "\n",
    "            classifier.fit(X.loc[:, s], y)\n",
    "\n",
    "            y_pred = classifier.predict(self.x_valid.loc[:, s])\n",
    "            score = f1(y_valid_reshaped, y_pred)\n",
    "\n",
    "            if score > f1_best:\n",
    "                f1_best = score\n",
    "                self.sample_best_ = s\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f\"Epoch {i+1}/{self.epochs}: F1 score \"\n",
    "                      f\"on validation set {round(score, 5)}\",\n",
    "                      flush=True)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _more_tags(self):\n",
    "        return {\"requires_y\": True}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common constants, model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Literal\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from hiclass import MultiLabelLocalClassifierPerNode\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "FeatureSelector = Union[ModSelectKBest, IterativeSelect]\n",
    "\n",
    "IMPUTER_STRATEGY = \"mean\"\n",
    "IMPUTER_KWARGS = {}\n",
    "MODEL_STEPS = [\n",
    "    # (\"model\", MultiLabelLocalClassifierPerNode(DecisionTreeClassifier())),\n",
    "    (\"model\", MultiLabelLocalClassifierPerNode(\n",
    "        local_classifier=LogisticRegression(\n",
    "            penalty='l2',\n",
    "            C=0.01,\n",
    "            solver='lbfgs',\n",
    "            max_iter=10000\n",
    "        )\n",
    "    ))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Dataset(\"cellcycle\", nan_strategy=\"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common functions (evaluation etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Dict, Optional, Tuple, Any\n",
    "\n",
    "\n",
    "def prep_dataset(dataset: Dataset,\n",
    "                 imputer_strategy: ImputerStrategy,\n",
    "                 imputer_kwargs: Optional[Dict[str, Any]] = None,\n",
    "                 x_train_prep: Optional[Callable[[pd.DataFrame], pd.DataFrame]] = None,\n",
    "                 ) -> Tuple[Dict[str, pd.DataFrame],\n",
    "                            NumericImputer]:\n",
    "    imputer = NumericImputer(strategy=imputer_strategy, **(imputer_kwargs or {}))\n",
    "    data: Dict[str, pd.DataFrame] = {}\n",
    "\n",
    "    data[\"x_train\"] = dataset.x_train()\n",
    "    if x_train_prep is not None:\n",
    "        data[\"x_train\"] = x_train_prep(data[\"x_train\"])\n",
    "    data[\"y_train\"] = dataset.y_train()\n",
    "    imputer.fit(data[\"x_train\"], data[\"y_train\"])\n",
    "\n",
    "    data[\"x_valid\"] = dataset.x_valid()\n",
    "    data[\"y_valid\"] = dataset.y_valid()\n",
    "    data[\"x_valid\"] = imputer.transform(data[\"x_valid\"])\n",
    "    \n",
    "    data[\"x_test\"] = dataset.x_test()\n",
    "    data[\"y_test\"] = dataset.y_test()\n",
    "    data[\"y_test_reshaped\"] = fill_reshape(data[\"y_test\"])\n",
    "    return data, imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict, List, Optional, Tuple\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.pipeline import Pipeline\n",
    "from hiclass.metrics import f1\n",
    "\n",
    "def evaluate(dataset: Optional[Dataset] = None,\n",
    "             data: Optional[Dict[str, pd.DataFrame]] = None,\n",
    "             model_steps: List[Tuple[str, BaseEstimator]] = [],\n",
    "             imputer_strategy: ImputerStrategy = \"mean\",\n",
    "             imputer_kwargs: Optional[Dict[str, Any]] = None,\n",
    "             x_train_prep: Optional[Callable[[pd.DataFrame], pd.DataFrame]] = None,\n",
    "             name: str = \"baseline\",\n",
    "             verbose: bool = True,\n",
    "             verbose_pipe: bool = False):\n",
    "    if data is None:\n",
    "        if dataset is None:\n",
    "            raise ValueError(\"Either dataset or data must be provided\")\n",
    "        data, imputer = prep_dataset(dataset,\n",
    "                                     imputer_strategy,\n",
    "                                     imputer_kwargs,\n",
    "                                     x_train_prep)\n",
    "    else:\n",
    "        imputer = NumericImputer(strategy=imputer_strategy, **(imputer_kwargs or {}))\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        (\"imputer\", imputer),\n",
    "        *model_steps,\n",
    "    ], verbose=verbose_pipe, memory=\"cache\")\n",
    "    \n",
    "    pipeline.fit(data[\"x_train\"], data[\"y_train\"])\n",
    "    y_pred = pipeline.predict(data[\"x_test\"])\n",
    "    micro_score = f1(data[\"y_test_reshaped\"], fill_reshape(y_pred), \"micro\")\n",
    "    macro_score = f1(data[\"y_test_reshaped\"], fill_reshape(y_pred), \"macro\")\n",
    "    if verbose:\n",
    "        print(f\"{name}: {micro_score:.4f} | {macro_score:.4f}\", flush=True)\n",
    "    return name, micro_score, macro_score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline: 0.4073 | 0.4044\n"
     ]
    }
   ],
   "source": [
    "base_name, base_micro_score, base_macro_score = evaluate(\n",
    "    dataset=d,\n",
    "    model_steps=MODEL_STEPS,\n",
    "    imputer_strategy=\"mean\",\n",
    "    verbose=False,\n",
    ")\n",
    "print(f\"{base_name}: {base_micro_score:.4f} | {base_macro_score:.4f}\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Iterable, Optional\n",
    "from itertools import product\n",
    "from pprint import PrettyPrinter\n",
    "\n",
    "\n",
    "pp = PrettyPrinter(indent=4)\n",
    "\n",
    "\n",
    "def feature_selection(dataset: Dataset,\n",
    "                      model_steps: List[Tuple[str, BaseEstimator]],\n",
    "                      imputer_strategy: ImputerStrategy,\n",
    "                      imputer_kwargs: Optional[Dict[str, Any]] = None,\n",
    "                      n_feature_splits: Optional[int] = None,\n",
    "                      n_features: Optional[Iterable[int]] = None,\n",
    "                      n_epochs: int | Iterable[int] = 100,\n",
    "                      verbose: bool = True,\n",
    "                      verbose_pipe: bool = False):\n",
    "    data, _ = prep_dataset(dataset, imputer_strategy, imputer_kwargs)\n",
    "    train_n_features = data[\"x_train\"].shape[1]\n",
    "    \n",
    "    if n_features is None:\n",
    "        if n_feature_splits is None:\n",
    "            n_feature_splits = 5\n",
    "\n",
    "        n_features = [round(k * train_n_features / (n_feature_splits + 1))\n",
    "                      for k\n",
    "                      in range(1, n_feature_splits + 1)]\n",
    "    elif any(not (0 < k <= train_n_features) for k in n_features):\n",
    "            raise ValueError(\"Invalid number of features\")\n",
    "    \n",
    "    if isinstance(n_epochs, int):\n",
    "        n_epochs = [n_epochs]\n",
    "    \n",
    "    selectors = (\n",
    "        (\"Select K-Best\",\n",
    "         [{\"k\": k} for k in n_features]),\n",
    "        (\"Iterative Random Select\",\n",
    "         [\n",
    "             {\n",
    "                 \"k\": k,\n",
    "                 \"x_valid\": data[\"x_valid\"],\n",
    "                 \"y_valid\": data[\"y_valid\"],\n",
    "                 \"epochs\": epochs,\n",
    "                 \"verbose\": verbose,\n",
    "             }\n",
    "             for k, epochs\n",
    "             in product(n_features, n_epochs)\n",
    "         ])\n",
    "    )\n",
    "    \n",
    "    for key, kwargs in selectors:\n",
    "        for kw in kwargs:\n",
    "            selector = (ModSelectKBest(**kw)\n",
    "                        if key == \"Select K-Best\"\n",
    "                        else IterativeSelect(**kw))\n",
    "            \n",
    "            name, micro_score, macro_score = evaluate(\n",
    "                data=data,\n",
    "                model_steps=[(\"selector\", selector), *model_steps],\n",
    "                imputer_strategy=imputer_strategy,\n",
    "                imputer_kwargs=imputer_kwargs,\n",
    "                name=f\"{key} ({kw.get('k', 'X')})\",\n",
    "                verbose=verbose,\n",
    "                verbose_pipe=verbose_pipe\n",
    "            )\n",
    "            yield name, micro_score, macro_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline            : 0.4073 | 0.4044\n"
     ]
    }
   ],
   "source": [
    "feat_sel_results = feature_selection(d,\n",
    "                                     MODEL_STEPS,\n",
    "                                     imputer_strategy=IMPUTER_STRATEGY,\n",
    "                                     imputer_kwargs=IMPUTER_KWARGS,\n",
    "                                     n_feature_splits=1,\n",
    "                                     n_epochs=1,\n",
    "                                     verbose=False)\n",
    "\n",
    "print(f\"{base_name:<20}: {base_micro_score:.4f} | {base_macro_score:.4f}\", flush=True)\n",
    "for sel_name, sel_micro_score, sel_macro_score in feat_sel_results:\n",
    "    print(f\"{sel_name:<20}: {sel_micro_score:.4f} | {sel_micro_score:.4f}\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noisy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from hiclass.MultiLabelLocalClassifierPerNode import MultiLabelLocalClassifierPerNode\n",
    "from hiclass.metrics import f1\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "def add_noise_to_train(train_df, percentage):\n",
    "    noisy_train = train_df.copy()\n",
    "    numeric_columns = train_df.select_dtypes(include=[np.number]).columns\n",
    "    noise = (train_df[numeric_columns].max() - train_df[numeric_columns].min()) * (percentage / 100)\n",
    "    noisy_train[numeric_columns] += noise\n",
    "    return noisy_train\n",
    "\n",
    "def add_nominal_noise(train_df, percentage):\n",
    "    noisy_df = train_df.copy()\n",
    "    nominal_columns = df.select_dtypes(include=['category', 'object']).columns\n",
    "    for col in nominal_columns:\n",
    "        if col == 'class':\n",
    "            continue\n",
    "        unique_values = df[col].apply(lambda x: tuple(x) if isinstance(x, list) else x).unique()\n",
    "        indices = noisy_df.sample(frac=percentage / 100).index\n",
    "        noisy_df.loc[indices, col] = np.random.choice(unique_values, size=len(indices))\n",
    "    return noisy_df\n",
    "\n",
    "class NoisePreprocessor(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, scaling_method='standard', outlier_method='zscore', outlier_threshold=3.0):\n",
    "        self.scaling_method = scaling_method\n",
    "        self.outlier_method = outlier_method\n",
    "        self.outlier_threshold = outlier_threshold\n",
    "        self.scaler_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if self.outlier_method in ['zscore', 'iqr', 'clip', 'mean']:\n",
    "            X_clean, _ = self._remove_outliers(X, y)\n",
    "        else:\n",
    "            X_clean = X\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "\n",
    "        if self.outlier_method in ['zscore', 'iqr', 'clip', 'mean']:\n",
    "            X, y = self._remove_outliers(X, y)\n",
    "        \n",
    "        if y is not None:\n",
    "            return X, y\n",
    "        return X\n",
    "\n",
    "    def _remove_outliers(self, df, y=None):\n",
    "        if self.outlier_method == 'zscore':\n",
    "            return self._remove_outliers_zscore(df, y)\n",
    "        elif self.outlier_method == 'iqr':\n",
    "            return self._remove_outliers_iqr(df, y)\n",
    "        elif self.outlier_method == 'clip':\n",
    "            return self._clip_outliers(df, y)\n",
    "        elif self.outlier_method == 'mean':\n",
    "            return self._replace_outliers_with_mean(df, y)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown outlier method: {self.outlier_method}\")\n",
    "\n",
    "    def _remove_outliers_zscore(self, df, y=None):\n",
    "        numeric_cols = df.select_dtypes(include=[np.number])\n",
    "        z_scores = np.abs(stats.zscore(numeric_cols))\n",
    "        filtered_entries = (z_scores < self.outlier_threshold).all(axis=1)\n",
    "        if y is not None:\n",
    "            return df.loc[filtered_entries], y.loc[filtered_entries]\n",
    "        return df.loc[filtered_entries], y\n",
    "\n",
    "    def _remove_outliers_iqr(self, df, y=None):\n",
    "        Q1 = df.quantile(0.25)\n",
    "        Q3 = df.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        filter = (df >= (Q1 - 1.5 * IQR)) & (df <= (Q3 + 1.5 * IQR))\n",
    "        filtered_entries = filter.all(axis=1)\n",
    "        if y is not None:\n",
    "            return df.loc[filtered_entries], y.loc[filtered_entries]\n",
    "        return df.loc[filtered_entries], y\n",
    "\n",
    "    def _clip_outliers(self, df, y=None, lower_percentile=0.01, upper_percentile=0.99):\n",
    "        lower_bound = df.quantile(lower_percentile)\n",
    "        upper_bound = df.quantile(upper_percentile)\n",
    "        df_clipped = df.clip(lower=lower_bound, upper=upper_bound, axis=1)\n",
    "        return df_clipped, y\n",
    "\n",
    "    def _replace_outliers_with_mean(self, df, y=None):\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        for col in numeric_cols:\n",
    "            mean = df[col].mean()\n",
    "            std = df[col].std()\n",
    "            outliers = np.abs((df[col] - mean) / std) > self.outlier_threshold\n",
    "            df.loc[outliers, col] = mean\n",
    "        return df, y\n",
    "\n",
    "    def _scale_data(self, df):\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        df.loc[:, numeric_cols] = self.scaler_.transform(df[numeric_cols])\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noisy_data(dataset: Dataset,\n",
    "                      model_steps: List[Tuple[str, BaseEstimator]],\n",
    "                      imputer_strategy: ImputerStrategy,\n",
    "                      imputer_kwargs: Optional[Dict[str, Any]] = None,\n",
    "                      verbose: bool = False):\n",
    "    for i in range(-10, 11):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        data, _ = prep_dataset(\n",
    "                    dataset,\n",
    "                    imputer_strategy,\n",
    "                    imputer_kwargs,\n",
    "                    x_train_prep=lambda x: add_noise_to_train(x, percentage=i)\n",
    "                )\n",
    "        \n",
    "        args = [\n",
    "            {'outlier_method': 'mean',\n",
    "             'scaling_method': None\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        for kw in args:\n",
    "            preprocessor = NoisePreprocessor(**kw)\n",
    "            \n",
    "            name, score = evaluate(\n",
    "                data=data,\n",
    "                model_steps=[(\"preprocessor\", preprocessor), *model_steps],\n",
    "                imputer_strategy=imputer_strategy,\n",
    "                imputer_kwargs=imputer_kwargs,\n",
    "                name=f\"numeric {i}%\",\n",
    "                verbose=verbose\n",
    "            )\n",
    "            yield name, score\n",
    "\n",
    "    for i in range(1, 11):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        data, _ = prep_dataset(\n",
    "                    dataset,\n",
    "                    imputer_strategy,\n",
    "                    imputer_kwargs,\n",
    "                    x_train_prep=lambda x: add_nominal_noise(x, percentage=i)\n",
    "                )\n",
    "        \n",
    "        args = [\n",
    "            {'outlier_method': 'mean',\n",
    "             'scaling_method': None\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        for kw in args:\n",
    "            preprocessor = NoisePreprocessor(**kw)\n",
    "            \n",
    "            name, score = evaluate(\n",
    "                data=data,\n",
    "                model_steps=[(\"preprocessor\", preprocessor), *model_steps],\n",
    "                imputer_strategy=imputer_strategy,\n",
    "                imputer_kwargs=imputer_kwargs,\n",
    "                name=f\"nominal {i}%\",\n",
    "                verbose=verbose\n",
    "            )\n",
    "            yield name, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline        : 0.4073\n",
      "numeric -10%    : 0.3602\n",
      "numeric -9%     : 0.3630\n",
      "numeric -8%     : 0.3649\n",
      "numeric -7%     : 0.3674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feat_sel_results = noisy_data(d,\n",
    "                             MODEL_STEPS,\n",
    "                             imputer_strategy=IMPUTER_STRATEGY,\n",
    "                             imputer_kwargs=IMPUTER_KWARGS,\n",
    "                             verbose=False)\n",
    "\n",
    "print(f\"{base_name:<16}: {base_f1_score:.4f}\", flush=True)\n",
    "for sel_name, f1_score in feat_sel_results:\n",
    "    print(f\"{sel_name:<16}: {f1_score:.4f}\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi-supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.semi_supervised import SelfTrainingClassifier\n",
    "from random import sample\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from hiclass.MultiLabelLocalClassifierPerNode import MultiLabelLocalClassifierPerNode\n",
    "from hiclass.metrics import f1\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def create_classifier():\n",
    "    \"\"\"\n",
    "    Create and configure a MultiLabelLocalClassifierPerNode with a self-training decision tree classifier.\n",
    "\n",
    "    Returns:\n",
    "    MultiLabelLocalClassifierPerNode: An instance of MultiLabelLocalClassifierPerNode configured\n",
    "                                      with a SelfTrainingClassifier that uses a DecisionTreeClassifier\n",
    "                                      as its base estimator. This setup is particularly suited for scenarios\n",
    "                                      where multilabel classification is required and some of the training\n",
    "                                      data might be unlabeled.\n",
    "    Sample usage:\n",
    "        import feature_selection\n",
    "        from hiclass.metrics import f1\n",
    "        from load import Dataset\n",
    "\n",
    "        dataset = Dataset(\"cellcycle\", nan_strategy=\"mean\")\n",
    "        x_test, y_test = dataset.x_test(), dataset.y_test()\n",
    "        x_train, y_train = dataset.x_train(), dataset.y_train()\n",
    "\n",
    "        classifier = create_classifier()\n",
    "\n",
    "        selector = feature_selection.ModSelectKBest().fit(x_train, y_train)\n",
    "        x_train = selector.transform(x_train)\n",
    "\n",
    "        classifier.fit(x_train, remove_labels(dataset.y_train(), percentage))\n",
    "\n",
    "        y_pred = classifier.predict(selector.transform(x_test))\n",
    "        print(f1(feature_selection.fill_reshape(y_test), y_pred))\n",
    "    \"\"\"\n",
    "    tree = DecisionTreeClassifier()\n",
    "    self_train = SelfTrainingClassifier(base_estimator=tree)\n",
    "    return MultiLabelLocalClassifierPerNode(local_classifier=self_train)\n",
    "\n",
    "\n",
    "def remove_labels(labels, percentage):\n",
    "    \"\"\"\n",
    "    Randomly sets a specified percentage of labels in the input list to [[-1]].\n",
    "\n",
    "    :param labels (list of list): A list of labels, where each label itself can be a list of items.\n",
    "    :param percentage (float): The fraction of labels to remove, represented as a float between 0 and 1.\n",
    "                        For example, 0.2 means 20% of the labels will be set to [[-1]].\n",
    "\n",
    "    Returns:\n",
    "    list of list of list: The modified list of labels with a percentage of its elements set to [[-1]].\n",
    "\n",
    "    Raises:\n",
    "    - ValueError: If 'percentage' is not between 0 and 1.\n",
    "    \"\"\"\n",
    "    if not (0 <= percentage <= 1):\n",
    "        raise ValueError(\"Percentage of unlabeled data must be between 0 and 1.\")\n",
    "    size = len(labels)\n",
    "    rand_indexes = sample(range(0, size), round(size*percentage))\n",
    "    for i in rand_indexes:\n",
    "        labels[i] = [[-1]]\n",
    "    return labels\n",
    "\n",
    "\n",
    "def train(dataset: Dataset, percentage: float):\n",
    "    \"\"\"\n",
    "    Create and run semi-supervised learner on the dataset with the percentage of unlabeled data.\n",
    "\n",
    "    :param dataset: An object representing the dataset.\n",
    "    :param percentage: The fraction of the training data to be treated as unlabeled, expressed as a\n",
    "      decimal (e.g., 1.0 for 100%, 0.2 for 20%).\n",
    "    :param feats: features to be selected from data\n",
    "\n",
    "    Sample usage:\n",
    "        from load import Dataset\n",
    "\n",
    "        train(Dataset(\"eisen\"), 0.1)\n",
    "    \"\"\"\n",
    "    x_test, y_test = dataset.x_test(), dataset.y_test()\n",
    "    x_train, y_train = dataset.x_train(), dataset.y_train()\n",
    "\n",
    "    classifier = create_classifier()\n",
    "\n",
    "    selector = ModSelectKBest().fit(x_train, y_train)\n",
    "    x_train = selector.transform(x_train)\n",
    "\n",
    "    classifier.fit(x_train, remove_labels(dataset.y_train(), percentage))\n",
    "\n",
    "    y_pred = classifier.predict(selector.transform(x_test))\n",
    "    return f1(fill_reshape(y_test), y_pred)\n",
    "    \n",
    "results = []\n",
    "percentages = [0.01, 0.025, 0.05, 0.075, 0.1]\n",
    "\n",
    "for percentage in percentages:\n",
    "    results.append(train(Dataset(\"derisi\"), percentage))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(percentages, results, marker='o')\n",
    "plt.title('Model Performance vs. Percentage of Unlabeled Data')\n",
    "plt.xlabel('Percentage of Unlabeled Data')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
